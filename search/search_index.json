{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Build an LLM RAG Chatbot With LangChain","text":"<p>You\u2019ve likely interacted with large language models (LLMs), like the ones behind OpenAI\u2019s ChatGPT, and experienced their remarkable ability to answer questions, summarize documents, write code, and much more. While LLMs are remarkable by themselves, with a little programming knowledge, you can leverage libraries like LangChain to create your own LLM-powered chatbots that can do just about anything.</p> <p>In an enterprise setting, one of the most popular ways to create an LLM-powered chatbot is through retrieval-augmented generation (RAG). When you design a RAG system, you use a retrieval model to retrieve relevant information, usually from a database or corpus, and provide this retrieved information to an LLM to generate contextually relevant responses.</p> <p>In this tutorial, you\u2019ll step into the shoes of an AI engineer working for a large hospital system. You\u2019ll build a RAG chatbot in LangChain that uses Neo4j to retrieve data about the patients, patient experiences, hospital locations, visits, insurance payers, and physicians in your hospital system.</p> <p>In this tutorial, you\u2019ll learn how to:</p> <ul> <li>Use LangChain to build custom chatbots</li> <li>Design a chatbot using your understanding of the business requirements and hospital system data</li> <li>Work with graph databases</li> <li>Set up a Neo4j AuraDB instance</li> <li>Build a RAG chatbot that retrieves both structured and unstructured data from Neo4j</li> <li>Deploy your chatbot with FastAPI and Streamlit."},{"location":"conclusion/","title":"Conclusion","text":"<p>Congratulations on completing this in-depth tutorial!</p> <p>You\u2019ve successfully designed, built, and served a RAG LangChain chatbot that answers questions about a fake hospital system. There are certainly many ways you can improve the chatbot you built in this tutorial, but you now have a sound understanding of how to integrate LangChain with your own data, giving you the creative freedom to build all kinds of custom chatbots.</p> <p>In this tutorial, you\u2019ve learned how to:</p> <ul> <li>Use LangChain to build personalized chatbots.</li> <li>Create a chatbot for a fake hospital system by aligning with business requirements and leveraging available data.</li> <li>Consider the implementation of graph databases in your chatbot design.</li> <li>Set up a Neo4j AuraDB instance for your project.</li> <li>Develop a RAG chatbot capable of fetching both structured and unstructured data from Neo4j.</li> <li>Deploy your chatbot using FastAPI and Streamlit.</li> </ul>"},{"location":"prerequisites/","title":"Prerequisites","text":"<p>This tutorial is best suited for intermediate Python developers who want to get hands-on experience creating custom chatbots. Aside from intermediate Python knowledge, you\u2019ll benefit from having a high-level understanding of the following concepts and technologies:</p> <ul> <li>Large language models (LLMs) and prompt engineering</li> <li>Text embeddings and vector databases</li> <li>Graph databases and Neo4j</li> <li>The OpenAI developer ecosystem</li> <li>REST APIs and FastAPI</li> <li>Asynchronous programming</li> <li>Docker and Docker Compose</li> </ul> <p>Nothing listed above is a hard prerequisite, so don\u2019t worry if you don\u2019t feel knowledgeable in any of them. You\u2019ll be introduced to each concept and technology along the way. Besides, there\u2019s no better way to learn these prerequisites than to implement them yourself in this tutorial.</p>"},{"location":"proejct-overview/","title":"Project Overview","text":"<p>Throughout this tutorial, you\u2019ll create a few directories that make up your final chatbot. Here\u2019s a breakdown of each directory:</p> <ul> <li> <p>langchain_intro/ will help you get familiar with LangChain and equip you with the tools that you need to build the chatbot you saw in the demo, and it won\u2019t be included in your final chatbot. You\u2019ll cover this in Step 1.</p> </li> <li> <p>data/ has the raw hospital system data stored as CSV files. You\u2019ll explore this data in Step 2. In Step 3, you\u2019ll move this data into a Neo4j database that your chatbot will query to answer questions.</p> </li> <li> <p>hospital_neo4j_etl/ contains a script that loads the raw data from data/ into your Neo4j database. You have to run this before building your chatbot, and you\u2019ll learn everything you need to know about setting up a Neo4j instance in Step 3.</p> </li> <li> <p>chatbot_api/ is your FastAPI app that serves your chatbot as a REST endpoint, and it\u2019s the core deliverable of this project. The chatbot_api/src/agents/ and chatbot_api/src/chains/ subdirectories contain the LangChain objects that comprise your chatbot. You\u2019ll learn what agents and chains are later, but for now, just know that your chatbot is actually a LangChain agent composed of chains and functions.</p> </li> <li> <p>tests/ includes two scripts that test how fast your chatbot can answer a series of questions. This will give you a feel for how much time you save by making asynchronous requests to LLM providers like OpenAI.</p> </li> <li> <p>chatbot_frontend/ is your Streamlit app that interacts with the chatbot endpoint in chatbot_api/. This is the UI that you saw in the demo, and you\u2019ll build this in Step 5.</p> </li> </ul> <p>All the environment variables needed to build and run your chatbot will be stored in a .env file. You\u2019ll deploy the code in hospital_neo4j_etl/, chatbot_api, and chatbot_frontend as Docker containers that\u2019ll be orchestrated with Docker Compose. If you want to experiment with the chatbot before going through the rest of this tutorial, then you can download the materials and follow the instructions in the README file to get things running:</p>"},{"location":"business/business/","title":"Introduction","text":"<p>Before you start working on any AI project, you need to understand the problem that you want to solve and make a plan for how you\u2019re going to solve it. This involves clearly defining the problem, gathering requirements, understanding the data and technology available to you, and setting clear expectations with stakeholders. For this project, you\u2019ll start by defining the problem and gathering business requirements for your chatbot.</p>"},{"location":"business/data/","title":"Explore the Available Data","text":"<p>Before building your chatbot, you need a thorough understanding of the data it will use to respond to user queries. This will help you determine what\u2019s feasible and how you want to structure the data so that your chatbot can easily access it. All of the data you\u2019ll use in this article was synthetically generated, and much of it was derived from a popular health care dataset on Kaggle.</p> <p>In practice, the following datasets would likely be stored as tables in a SQL database, but you\u2019ll work with CSV files to keep the focus on building the chatbot. This section will give you a detailed description of each CSV file.</p> <p>You\u2019ll need to place all CSV files that are part of this project in your data/ folder before continuing with the tutorial. Make sure that you downloaded them from the materials and placed them in your data/ folder:</p>"},{"location":"business/data/#hospitalscsv","title":"hospitals.csv","text":"<p>The hospitals.csv file records information on each hospital that your company manages. There 30 hospitals and three fields in this file:</p> <ul> <li>hospital_id: An integer that uniquely identifies a hospital.</li> <li>hospital_name: The hospital\u2019s name.</li> <li>hospital_state: The state the hospital is located in.</li> </ul> <p>If you\u2019re familiar with traditional SQL databases and the star schema, you can think of hospitals.csv as a dimension table. Dimension tables are relatively short and contain descriptive information or attributes that provide context to the data in fact tables. Fact tables record events about the entities stored in dimension tables, and they tend to be longer tables.</p> <p>In this case, hospitals.csv records information specific to hospitals, but you can join it to fact tables to answer questions about which patients, physicians, and payers are related to the hospital. This will be more clear when you explore visits.csv.</p> <p>If you\u2019re curious, you can inspect the first few rows of hospitals.csv using a dataframe library like Polars. Make sure Polars is installed in your virtual environment, and run the following code:</p> <pre><code>import polars as pl\n\nHOSPITAL_DATA_PATH = \"/Users/i552839/Desktop/Reddy/LLM-RAG-Chatbot/LLM-RAG-Chatbot/langchain/data/hospitals.csv\"\ndata_hospitals = pl.read_csv(HOSPITAL_DATA_PATH)\n\nprint(data_hospitals.shape)\n\ndf = data_hospitals.head()\n\nprint(df)\n</code></pre> <p>In this code block, you import Polars, define the path to hospitals.csv, read the data into a Polars DataFrame, display the shape of the data, and display the first 5 rows. This shows you, for example, that Walton, LLC hospital has an ID of 2 and is located in the state of Florida, FL.</p>"},{"location":"business/data/#physicianscsv","title":"physicians.csv","text":"<p>The physicians.csv file contains data about the physicians that work for your hospital system. This dataset has the following fields:</p> <p>physician_id: An integer that uniquely identifies each physician. physician_name: The physician\u2019s name. physician_dob: The physician\u2019s date of birth. physician_grad_year: The year the physician graduated medical school. medical_school: Where the physician attended medical school. salary: The physician\u2019s salary. This data can again be thought of as a dimension table, and you can inspect the first few rows using Polars:</p> <pre><code>PHYSICIAN_DATA_PATH = \"data/physicians.csv\"\ndata_physician = pl.read_csv(PHYSICIAN_DATA_PATH)\n\nprint(data_physician.shape)\n\n\nprint(data_physician.head())\n</code></pre> <p>As you can see from the code block, there are 500 physicians in physicians.csv. The first few rows from physicians.csv give you a feel for what the data looks like. For instance, Heather Smith has a physician ID of 3, was born on June 15, 1965, graduated medical school on June 15, 1995, attended NYU Grossman Medical School, and her salary is about $295,239.</p>"},{"location":"business/data/#payerscsv","title":"payers.csv","text":"<p>The next file, payers.csv, records information about the insurance companies that your hospitals bills for patient visits. Similar to hospitals.csv, it\u2019s a small file with a couple fields:</p> <p>payer_id: An integer that uniquely identifies each payer. payer_name: The payer\u2019s company name. The only five payers in the data are Medicaid, UnitedHealthcare, Aetna, Cigna, and Blue Cross. Your stakeholders are very interested in payer activity, so payers.csv will be helpful once it\u2019s connected to patients, hospitals, and physicians.</p>"},{"location":"business/data/#reviewscsv","title":"reviews.csv","text":"<p>The reviews.csv file contains patient reviews about their experience at the hospital. It has these fields:</p> <ul> <li>review_id: An integer that uniquely identifies a review.</li> <li>visit_id: An integer that identifies the patient\u2019s visit that the review was about.</li> <li>review: This is the free form text review left by the patient.</li> <li>physician_name: The name of the physician who treated the patient.</li> <li>hospital_name: The hospital where the patient stayed.</li> <li>patient_name: The patient\u2019s name.</li> </ul> <p>This dataset is the first one you\u2019ve seen that contains the free text review field, and your chatbot should use this to answer questions about review details and patient experiences.</p> <p>Here\u2019s what reviews.csv looks like:</p> <pre><code>REVIEWS_DATA_PATH = \"data/reviews.csv\"\ndata_reviews = pl.read_csv(REVIEWS_DATA_PATH)\n\nprint(data_reviews.shape)\n\n\nprint(data_reviews.head())\n</code></pre>"},{"location":"business/data/#visitscsv","title":"visits.csv","text":"<p>The last file, visits.csv, records details about every hospital visit your company has serviced. Continuing with the star schema analogy, you can think of visits.csv as a fact table that connects hospitals, physicians, patients, and payers. Here are the fields:</p> <ul> <li>visit_id: The unique identifier of a hospital visit.</li> <li>patient_id: The ID of the patient associated with the visit.</li> <li>date_of_admission: The date the patient was admitted to the hospital.</li> <li>room_number: The patient\u2019s room number.</li> <li>admission_type: One of \u2018Elective\u2019, \u2018Emergency\u2019, or \u2018Urgent\u2019.</li> <li>chief_complaint: A string describing the patient\u2019s primary reason for being at the hospital.</li> <li>primary_diagnosis: A string describing the primary diagnosis made by the physician.</li> <li>treatment_description: A text summary of the treatment given by the physician.</li> <li>test_results: One of \u2018Inconclusive\u2019, \u2018Normal\u2019, or \u2018Abnormal\u2019.</li> <li>discharge_date: The date the patient was discharged from the hospital</li> <li>physician_id: The ID of the physician that treated the patient.</li> <li>hospital_id: The ID of the hospital the patient stayed at.</li> <li>payer_id: The ID of the insurance payer used by the patient.</li> <li>billing_amount: The amount of money billed to the payer for the visit.</li> <li>visit_status: One of \u2018OPEN\u2019 or \u2018DISCHARGED\u2019.</li> </ul> <p>This dataset gives you everything you need to answer questions about the relationship between each hospital entity. For example, if you know a physician ID, you can use visits.csv to figure out which patients, payers, and hospitals the physician is associated with. Take a look at what visits.csv looks like in Polars:</p> <pre><code>VISITS_DATA_PATH = \"data/visits.csv\"\ndata_visits = pl.read_csv(VISITS_DATA_PATH)\nprint(data_visits.shape)\nprint(data_visits.head())\n</code></pre> <p>You can see there are 9998 visits recorded along with the 15 fields described above. Notice that chief_complaint, treatment_description, and primary_diagnosis might be missing for a visit. You\u2019ll have to keep this in mind as your stakeholders might not be aware that many visits are missing critical data\u2014this may be a valuable insight in itself! Lastly, notice that when a visit is still open, the discharged_date will be missing.</p> <p>You now have an understanding of the data you\u2019ll use to build the chatbot your stakeholders want. To recap, the files are broken out to simulate what a traditional SQL database might look like. Every hospital, patient, physician, review, and payer are connected through visits.csv.</p>"},{"location":"business/data/#wait-times","title":"Wait Times","text":"<p>You might have noticed there\u2019s no data to answer questions like What is the current wait time at XYZ hospital? or Which hospital currently has the shortest wait time?. Unfortunately, the hospital system doesn\u2019t record historical wait times. Your chatbot will have to call an API to get current wait time information. You\u2019ll see how this works later.</p> <p>With an understanding of the business requirements, available data, and LangChain functionalities, you can create a design for your chatbot.</p>"},{"location":"business/design_chatbot/","title":"Design the Chatbot","text":"<p>Now that you know the business requirements, data, and LangChain prerequisites, you\u2019re ready to design your chatbot. A good design gives you and others a conceptual understanding of the components needed to build your chatbot. Your design should clearly illustrate how data flows through your chatbot, and it should serve as a helpful reference during development.</p> <p>Your chatbot will use multiple tools to answer diverse questions about your hospital system. Here\u2019s a flowchart illustrating how you\u2019ll accomplish this:</p> <p></p> <p>This flowchart illustrates how data moves through your chatbot, starting from the user\u2019s input query all the way to the final response. Here\u2019s a summary of each component:</p> <p>LangChain Agent: The LangChain agent is the brain of your chatbot. Given a user query, the agent decides which tool to call and what to give the tool as input. The agent then observes the tool\u2019s output and decides what to return to the user\u2014this is the agent\u2019s response.</p> <p>Neo4j AuraDB: You\u2019ll store both structured hospital system data and patient reviews in a Neo4j AuraDB graph database. You\u2019ll learn all about this in the next section.</p> <p>LangChain Neo4j Cypher Chain: This chain tries to convert the user query into Cypher, Neo4j\u2019s query language, and execute the Cypher query in Neo4j. The chain then answers the user query using the Cypher query results. The chain\u2019s response is fed back to the LangChain agent and sent to the user.</p> <p>LangChain Neo4j Reviews Vector Chain: This is very similar to the chain you built in Step 1, except now patient review embeddings are stored in Neo4j. The chain searches for relevant reviews based on those semantically similar to the user query, and the reviews are used to answer the user query.</p> <p>Wait Times Function: Similar to the logic in Step 1, the LangChain agent tries to extract a hospital name from the user query. The hospital name is passed as input to a Python function that gets wait times, and the wait time is returned to the agent.</p> <p>To walk through an example, suppose a user asks How many emergency visits were there in 2023? The LangChain agent will receive this question and decide which tool, if any, to pass the question to. In this case, the agent should pass the question to the LangChain Neo4j Cypher Chain. The chain will try to convert the question to a Cypher query, run the Cypher query in Neo4j, and use the query results to answer the question.</p> <p>Once the LangChain Neo4j Cypher Chain answers the question, it will return the answer to the agent, and the agent will relay the answer to the user.</p> <p>With this design in mind, you can start building your chatbot. Your first task is to set up a Neo4j AuraDB instance for your chatbot to access.</p>"},{"location":"business/problem_requirements/","title":"Understand the Problem and Requirements","text":"<p>Imagine you\u2019re an AI engineer working for a large hospital system in the US. Your stakeholders would like more visibility into the ever-changing data they collect. They want answers to ad-hoc questions about patients, visits, physicians, hospitals, and insurance payers without having to understand a query language like SQL, request a report from an analyst, or wait for someone to build a dashboard.</p> <p>To accomplish this, your stakeholders want an internal chatbot tool, similar to ChatGPT, that can answer questions about your company\u2019s data. After meeting to gather requirements, you\u2019re provided with a list of the kinds of questions your chatbot should answer:</p> <ul> <li>What is the current wait time at XYZ hospital?</li> <li>Which hospital currently has the shortest wait time?</li> <li>At which hospitals are patients complaining about billing and insurance issues?</li> <li>Have any patients complained about the hospital being unclean?</li> <li>What have patients said about how doctors and nurses communicate with them?</li> <li>What are patients saying about the nursing staff at XYZ hospital?</li> <li>What was the total billing amount charged to Cigna payers in 2023?</li> <li>How many patients has Dr. John Doe treated?</li> <li>How many visits are open and what is their average duration in days?</li> <li>Which physician has the lowest average visit duration in days?</li> <li>How much was billed for patient 789\u2019s stay?</li> <li>Which hospital worked with the most Cigna patients in 2023?</li> <li>What\u2019s the average billing amount for emergency visits by hospital?</li> <li>Which state had the largest percent increase inedicaid visits from 2022 to 2023?</li> </ul> <p>You can answer questions like What was the total billing amount charged to Cigna payers in 2023? with aggregate statistics using a query language like SQL. Crucially, these questions have a single objective answer. You could run pre-defined queries to answer these, but any time a stakeholder has a new or slightly nuanced question, you have to write a new query. To avoid this, your chatbot should dynamically generate accurate queries.</p> <p>Questions like Have any patients complained about the hospital being unclean? or What have patients said about how doctors and nurses communicate with them? are more subjective and might have many acceptable answers. Your chatbot will need to read through documents, such as patient reviews, to answer these kinds of questions.</p> <p>Ultimately, your stakeholders want a single chat interface that can seamlessly answer both subjective and objective questions. This means, when presented with a question, your chatbot needs to know what type of question is being asked and which data source to pull from.</p> <p>For instance, if asked How much was billed for patient 789\u2019s stay?, your chatbot should know it needs to query a database to find the answer. If asked What have patients said about how doctors and nurses communicate with them?, your chatbot should know it needs to read and summarize patient reviews.</p> <p>Next up, you\u2019ll explore the data your hospital system records, which is arguably the most important prerequisite to building your chatbot.</p>"},{"location":"deployment/chat_ui/","title":"Create a Chat UI With Streamlit","text":"<p>Your stakeholders need a way to interact with your agent without making manual API requests. To accommodate this, you\u2019ll build a Streamlit app that acts as an interface between your stakeholders and your API. Here are the dependencies for the Streamlit UI:</p> <pre><code>[tool.poetry]\nname = \"chatbot_frontend\"\nversion = \"0.1\"\n[tool.poetry.dependencies]\nrequests=\"^2.31.0\"\nstreamlit=\"^1.29.0\"\n\n[tool.poetry.group.dev.dependencies]\nblack = \"^24.3.0\"\nflake8 = \"^7.0.0\"\n</code></pre> <p>The driving code for your Streamlit app is in chatbot_frontend/src/main.py:</p> <pre><code>import os\nimport requests\nimport streamlit as st\n\nCHATBOT_URL = os.getenv(\"CHATBOT_URL\", \"http://localhost:8000/hospital-rag-agent\")\n\nwith st.sidebar:\n    st.header(\"About\")\n    st.markdown(\n        \"\"\"\n        This chatbot interfaces with a\n        [LangChain](https://python.langchain.com/docs/get_started/introduction)\n        agent designed to answer questions about the hospitals, patients,\n        visits, physicians, and insurance payers in  a fake hospital system.\n        The agent uses  retrieval-augment generation (RAG) over both\n        structured and unstructured data that has been synthetically generated.\n        \"\"\"\n    )\n\n    st.header(\"Example Questions\")\n    st.markdown(\"- Which hospitals are in the hospital system?\")\n    st.markdown(\"- What is the current wait time at wallace-hamilton hospital?\")\n    st.markdown(\n        \"- At which hospitals are patients complaining about billing and \"\n        \"insurance issues?\"\n    )\n    st.markdown(\"- What is the average duration in days for closed emergency visits?\")\n    st.markdown(\n        \"- What are patients saying about the nursing staff at \"\n        \"Castaneda-Hardy?\"\n    )\n    st.markdown(\"- What was the total billing amount charged to each payer for 2023?\")\n    st.markdown(\"- What is the average billing amount for medicaid visits?\")\n    st.markdown(\"- Which physician has the lowest average visit duration in days?\")\n    st.markdown(\"- How much was billed for patient 789's stay?\")\n    st.markdown(\n        \"- Which state had the largest percent increase in medicaid visits \"\n        \"from 2022 to 2023?\"\n    )\n    st.markdown(\"- What is the average billing amount per day for Aetna patients?\")\n    st.markdown(\"- How many reviews have been written from patients in Florida?\")\n    st.markdown(\n        \"- For visits that are not missing chief complaints, \"\n        \"what percentage have reviews?\"\n    )\n    st.markdown(\n        \"- What is the percentage of visits that have reviews for each hospital?\"\n    )\n    st.markdown(\n        \"- Which physician has received the most reviews for this visits \"\n        \"they've attended?\"\n    )\n    st.markdown(\"- What is the ID for physician James Cooper?\")\n    st.markdown(\n        \"- List every review for visits treated by physician 270. Don't leave any out.\"\n    )\n\nst.title(\"Hospital System Chatbot\")\nst.info(\n    \"Ask me questions about patients, visits, insurance payers, hospitals, \"\n    \"physicians, reviews, and wait times!\"\n)\n\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        if \"output\" in message.keys():\n            st.markdown(message[\"output\"])\n\n        if \"explanation\" in message.keys():\n            with st.status(\"How was this generated\", state=\"complete\"):\n                st.info(message[\"explanation\"])\n\nif prompt := st.chat_input(\"What do you want to know?\"):\n    st.chat_message(\"user\").markdown(prompt)\n\n    st.session_state.messages.append({\"role\": \"user\", \"output\": prompt})\n\n    data = {\"text\": prompt}\n\n    with st.spinner(\"Searching for an answer...\"):\n        response = requests.post(CHATBOT_URL, json=data)\n\n        if response.status_code == 200:\n            output_text = response.json()[\"output\"]\n            explanation = response.json()[\"intermediate_steps\"]\n\n        else:\n            output_text = \"\"\"An error occurred while processing your message.\n            Please try again or rephrase your message.\"\"\"\n            explanation = output_text\n\n    st.chat_message(\"assistant\").markdown(output_text)\n    st.status(\"How was this generated\", state=\"complete\").info(explanation)\n\n    st.session_state.messages.append(\n        {\n            \"role\": \"assistant\",\n            \"output\": output_text,\n            \"explanation\": explanation,\n        }\n    )\n</code></pre> <p>Learning Streamlit is not the focus of this tutorial, so you won\u2019t get a detailed description of this code. However, here\u2019s a high-level overview of what this UI does:</p> <ul> <li>The entire chat history is stored and displayed each time the user makes a new query.</li> <li>The UI takes the user\u2019s input and makes a synchronous POST request to the agent endpoint.</li> <li>The most recent agent response is displayed at the bottom of the chat and appended to the chat history.</li> <li>An explanation of how the agent generated its response it provided to the user. This is great for auditing purposes because you can see if the agent called the right tool, and you can check if the tool worked correctly.</li> </ul> <p>As you\u2019ve done, you\u2019ll create an entrypoint file to run the UI:</p> <pre><code>#!/bin/bash\n\n# Run any setup steps or pre-processing tasks here\necho \"Starting hospital chatbot frontend...\"\n\n# Run the ETL script\nstreamlit run main.py\n</code></pre> <p>And finally, the Docker file to create an image for the UI:</p> <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY ./src/ /app\n\nCOPY ./pyproject.toml /code/pyproject.toml\nRUN pip install /code/.\n\nCMD [\"sh\", \"entrypoint.sh\"]\n</code></pre> <p>This Dockerfile is identical to the previous ones you\u2019ve created. With that, you\u2019re ready to run your entire chatbot application end-to-end.</p>"},{"location":"deployment/fastapi/","title":"Serve the Agent With FastAPI","text":"<p>FastAPI is a modern, high-performance web framework for building APIs with Python based on standard type hints. It comes with a lot of great features including development speed, runtime speed, and great community support, making it a great choice for serving your chatbot agent.</p> <p>You\u2019ll serve your agent through a POST request, so the first step is to define what data you expect to get in the request body and what data the request returns. FastAPI does this with Pydantic:</p> <pre><code>from pydantic import BaseModel\n\nclass HospitalQueryInput(BaseModel):\n    text: str\n\nclass HospitalQueryOutput(BaseModel):\n    input: str\n    output: str\n    intermediate_steps: list[str]\n</code></pre> <p>In this script, you define Pydantic models HospitalQueryInput and HospitalQueryOutput. HospitalQueryInput is used to verify that the POST request body includes a text field, representing the query your chatbot responds to. HospitalQueryOutput verifies the response body sent back to your user includes input, output, and intermediate_step fields.</p> <p>One great feature of FastAPI is its asynchronous serving capabilities. Because your agent calls OpenAI models hosted on an external server, there will always be latency while your agent waits for a response. This is a perfect opportunity for you to use asynchronous programming.</p> <p>Instead of waiting for OpenAI to respond to each of your agent\u2019s requests, you can have your agent make multiple requests in a row and store the responses as they\u2019re received. This will save you a lot of time if you have multiple queries you need your agent to respond to.</p> <p>As discussed previously, there can sometimes be intermittent connection issues with Neo4j that are usually resolved by establishing a new connection. Because of this, you\u2019ll want to implement retry logic that works for asynchronous functions:</p> <pre><code>import asyncio\n\ndef async_retry(max_retries: int=3, delay: int=1):\n    def decorator(func):\n        async def wrapper(*args, **kwargs):\n            for attempt in range(1, max_retries + 1):\n                try:\n                    result = await func(*args, **kwargs)\n                    return result\n                except Exception as e:\n                    print(f\"Attempt {attempt} failed: {str(e)}\")\n                    await asyncio.sleep(delay)\n\n            raise ValueError(f\"Failed after {max_retries} attempts\")\n\n        return wrapper\n\n    return decorator\n</code></pre> <p>Don\u2019t worry about the details of @async_retry. All you need to know is that it will retry an asynchronous function if it fails. You\u2019ll see where this is used next.</p> <p>The driving logic for your chatbot API is in chatbot_api/src/main.py:</p> <pre><code>from fastapi import FastAPI\nfrom agents.hospital_rag_agent import hospital_rag_agent_executor\nfrom models.hospital_rag_query import HospitalQueryInput, HospitalQueryOutput\nfrom utils.async_utils import async_retry\n\napp = FastAPI(\n    title=\"Hospital Chatbot\",\n    description=\"Endpoints for a hospital system graph RAG chatbot\",\n)\n\n@async_retry(max_retries=10, delay=1)\nasync def invoke_agent_with_retry(query: str):\n    \"\"\"Retry the agent if a tool fails to run.\n\n    This can help when there are intermittent connection issues\n    to external APIs.\n    \"\"\"\n    return await hospital_rag_agent_executor.ainvoke({\"input\": query})\n\n@app.get(\"/\")\nasync def get_status():\n    return {\"status\": \"running\"}\n\n@app.post(\"/hospital-rag-agent\")\nasync def query_hospital_agent(query: HospitalQueryInput) -&gt; HospitalQueryOutput:\n    query_response = await invoke_agent_with_retry(query.text)\n    query_response[\"intermediate_steps\"] = [\n        str(s) for s in query_response[\"intermediate_steps\"]\n    ]\n\n    return query_response\n</code></pre> <p>You import FastAPI, your agent executor, the Pydantic models you created for the POST request, and @async_retry. Then you instantiate a FastAPI object and define invoke_agent_with_retry(), a function that runs your agent asynchronously. The @async_retry decorator above invoke_agent_with_retry() ensures the function will be retried ten times with a delay of one second before failing.</p> <p>Lastly, you define query_hospital_agent() which serves POST requests to your agent at /hospital-rag-agent. This function extracts the text field from the request body, passes it to the agent, and returns the agent\u2019s response to the user.</p> <p>You\u2019ll serve this API with Docker and you\u2019ll want to define the following entrypoint file to run inside the container:</p> <pre><code>#!/bin/bash\n\n# Run any setup steps or pre-processing tasks here\necho \"Starting hospital RAG FastAPI service...\"\n\n# Start the main application\nuvicorn main:app --host 0.0.0.0 --port 8000\n</code></pre> <p>The command uvicorn main:app --host 0.0.0.0 --port 8000 runs the FastAPI application at port 8000 on your machine. The driving Dockerfile for your FastAPI app looks like this:</p> <pre><code># chatbot_api/Dockerfile\n\nFROM python:3.12-slim\n\nWORKDIR /app\nCOPY ./src/ /app\n\nCOPY ./pyproject.toml /code/pyproject.toml\nRUN pip install /code/.\n\nEXPOSE 8000\nCMD [\"sh\", \"entrypoint.sh\"]\n</code></pre> <p>This Dockerfile tells your container to use the python:3.12-slim distribution, copy the contents from chatbot_api/src/ into the /app directory within the container, install the dependencies from pyproject.toml, and run entrypoint.sh.</p> <p>The last thing you\u2019ll need to do is update the docker-compose.yml file to include your FastAPI container:</p> <pre><code>version: '3'\n\nservices:\n  hospital_neo4j_etl:\n    build:\n      context: ./hospital_neo4j_etl\n    env_file:\n      - .env\n\n  chatbot_api:\n    build:\n      context: ./chatbot_api\n    env_file:\n      - .env\n    depends_on:\n      - hospital_neo4j_etl\n    ports:\n      - \"8000:8000\"\n</code></pre> <p>Here you add the chatbot_api service which is derived from the Dockerfile in ./chatbot_api. It depends on hospital_neo4j_etl and will run on port 8000.</p> <p>To run the API, along with the ETL you build earlier, open a terminal and run:</p> <pre><code>docker-compose up --build\n</code></pre> <p>If everything runs successfully, you\u2019ll see a screen similar to the following at http://localhost:8000/docs#/: </p> <p>You can use the docs page to test the hospital-rag-agent endpoint, but you won\u2019t be able to make asynchronous requests here. To see how your endpoint handles asynchronous requests, you can test it with a library like httpx.</p> <p>Note: You need to install httpx into your virtual environment before running the tests below.</p> <p>To see how much time asynchronous requests save you, start by establishing a benchmark using synchronous requests. Create the following script:</p> <pre><code>import asyncio\nimport time\nimport httpx\n\nCHATBOT_URL = \"http://localhost:8000/hospital-rag-agent\"\n\nasync def make_async_post(url, data):\n    timeout = httpx.Timeout(timeout=120)\n    async with httpx.AsyncClient() as client:\n        response = await client.post(url, json=data, timeout=timeout)\n        return response\n\nasync def make_bulk_requests(url, data):\n    tasks = [make_async_post(url, payload) for payload in data]\n    responses = await asyncio.gather(*tasks)\n    outputs = [r.json()[\"output\"] for r in responses]\n    return outputs\n\nquestions = [\n   \"What is the current wait time at Wallace-Hamilton hospital?\",\n   \"Which hospital has the shortest wait time?\",\n   \"At which hospitals are patients complaining about billing and insurance issues?\",\n   \"What is the average duration in days for emergency visits?\",\n   \"What are patients saying about the nursing staff at Castaneda-Hardy?\",\n   \"What was the total billing amount charged to each payer for 2023?\",\n   \"What is the average billing amount for Medicaid visits?\",\n   \"How many patients has Dr. Ryan Brown treated?\",\n   \"Which physician has the lowest average visit duration in days?\",\n   \"How many visits are open and what is their average duration in days?\",\n   \"Have any patients complained about noise?\",\n   \"How much was billed for patient 789's stay?\",\n   \"Which physician has billed the most to cigna?\",\n   \"Which state had the largest percent increase in Medicaid visits from 2022 to 2023?\",\n]\n\nrequest_bodies = [{\"text\": q} for q in questions]\n\nstart_time = time.perf_counter()\noutputs = asyncio.run(make_bulk_requests(CHATBOT_URL, request_bodies))\nend_time = time.perf_counter()\n\nprint(f\"Run time: {end_time - start_time} seconds\")\n</code></pre> <p>In this script, you import requests and time, define the URL to your chatbot, create a list of questions, and record the amount of time it takes to get a response to all the questions in the list. If you open a terminal and run sync_agent_requests.py, you\u2019ll see how long it takes to answer all 14 questions:</p> <pre><code>python tests/sync_agent_requests.py\n</code></pre> <p>You may get slightly different results depending on your Internet speed and the availability of the chat model, but you can see this script took around 68 seconds to run. Next, you\u2019ll get answers to the same questions asynchronously:</p> <pre><code>import asyncio\nimport time\nimport httpx\n\nCHATBOT_URL = \"http://localhost:8000/hospital-rag-agent\"\n\nasync def make_async_post(url, data):\n    timeout = httpx.Timeout(timeout=120)\n    async with httpx.AsyncClient() as client:\n        response = await client.post(url, json=data, timeout=timeout)\n        return response\n\nasync def make_bulk_requests(url, data):\n    tasks = [make_async_post(url, payload) for payload in data]\n    responses = await asyncio.gather(*tasks)\n    outputs = [r.json()[\"output\"] for r in responses]\n    return outputs\n\nquestions = [\n   \"What is the current wait time at Wallace-Hamilton hospital?\",\n   \"Which hospital has the shortest wait time?\",\n   \"At which hospitals are patients complaining about billing and insurance issues?\",\n   \"What is the average duration in days for emergency visits?\",\n   \"What are patients saying about the nursing staff at Castaneda-Hardy?\",\n   \"What was the total billing amount charged to each payer for 2023?\",\n   \"What is the average billing amount for Medicaid visits?\",\n   \"How many patients has Dr. Ryan Brown treated?\",\n   \"Which physician has the lowest average visit duration in days?\",\n   \"How many visits are open and what is their average duration in days?\",\n   \"Have any patients complained about noise?\",\n   \"How much was billed for patient 789's stay?\",\n   \"Which physician has billed the most to cigna?\",\n   \"Which state had the largest percent increase in Medicaid visits from 2022 to 2023?\",\n]\n\nrequest_bodies = [{\"text\": q} for q in questions]\n\nstart_time = time.perf_counter()\noutputs = asyncio.run(make_bulk_requests(CHATBOT_URL, request_bodies))\nend_time = time.perf_counter()\n\nprint(f\"Run time: {end_time - start_time} seconds\")\n</code></pre> <p>In async_agent_requests.py, you make the same request you did in sync_agent_requests.py, except now you use httpx to make the requests asynchronously. Here are the results:</p> <pre><code>python tests/async_agent_requests.py\n</code></pre> <p>Again, the exact time this takes to run may vary for you, but you can see making 14 requests asynchronously was roughly four times faster. Deploying your agent asynchronously allows you to scale to a high-request volume without having to increase your infrastructure demands. While there are always exceptions, serving REST endpoints asynchronously is usually a good idea when your code makes network-bound requests.</p> <p>With this FastAPI endpoint functioning, you\u2019ve made your agent accessible to anyone who can access the endpoint. This is great for integrating your agent into chatbot UIs, which is what you\u2019ll do next with Streamlit.</p>"},{"location":"deployment/introduction/","title":"Deploy the LangChain Agent","text":"<p>At long last, you have a functioning LangChain agent that serves as your hospital system chatbot. The last thing you need to do is get your chatbot in front of stakeholders. For this, you\u2019ll deploy your chatbot as a FastAPI endpoint and create a Streamlit UI to interact with the endpoint.</p> <p>Before you get started, create two new folders called chatbot_frontend/ and tests/ in your project\u2019s root directory. You\u2019ll also need to add some additional files and folders to chatbot_api/:</p> <pre><code>./\n\u2502\n\u251c\u2500\u2500 chatbot_api/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 agents/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 hospital_rag_agent.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 chains/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 hospital_cypher_chain.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 hospital_review_chain.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 hospital_rag_query.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 tools/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 wait_times.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 async_utils.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2502   \u2514\u2500\u2500 main.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 chatbot_frontend/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2502   \u2514\u2500\u2500 main.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 hospital_neo4j_etl/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2502   \u2514\u2500\u2500 hospital_bulk_csv_write.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 async_agent_requests.py\n\u2502   \u2514\u2500\u2500 sync_agent_requests.py\n\u2502\n\u251c\u2500\u2500 .env\n</code></pre> <p>You need the new files in chatbot_api to build your FastAPI app, and tests/ has two scripts to demonstrate the power of making asynchronous requests to your agent. Lastly, chatbot_frontend/ has the code for the Streamlit UI that\u2019ll interface with your chatbot. You\u2019ll start by creating a FastAPI application to serve your agent.</p>"},{"location":"deployment/orck/","title":"Orchestrate the Project With Docker Compose","text":"<p>At this point, you\u2019ve written all the code needed to run your chatbot. This last step is to build and run your project with docker-compose. Before doing so, make sure your have all of the following files and folders in your project directory:</p> <pre><code>./\n\u2502\n\u251c\u2500\u2500 chatbot_api/\n\u2502   \u2502\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 agents/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 hospital_rag_agent.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 chains/\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 hospital_cypher_chain.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 hospital_review_chain.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 hospital_rag_query.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 tools/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 wait_times.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 async_utils.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2502   \u2514\u2500\u2500 main.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 chatbot_frontend/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2502   \u2514\u2500\u2500 main.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 hospital_neo4j_etl/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2502   \u2514\u2500\u2500 hospital_bulk_csv_write.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 async_agent_requests.py\n\u2502   \u2514\u2500\u2500 sync_agent_requests.py\n\u2502\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 docker-compose.yml\n</code></pre> <p>Your .env file should have the following environment variables. Most of them you created earlier in this tutorial, but you\u2019ll also need to add one new one for CHATBOT_URL so that your Streamlit app will be able to find your API:</p> <pre><code>....\n\nNEO4J_URI=&lt;YOUR_NEO4J_URI&gt;\nNEO4J_USERNAME=&lt;YOUR_NEO4J_USERNAME&gt;\nNEO4J_PASSWORD=&lt;YOUR_NEO4J_PASSWORD&gt;\n\nHOSPITALS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/hospitals.csv\nPAYERS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/payers.csv\nPHYSICIANS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/physicians.csv\nPATIENTS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/patients.csv\nVISITS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/visits.csv\nREVIEWS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/reviews.csv\n\nHOSPITAL_AGENT_MODEL=gpt-3.5-turbo-1106\nHOSPITAL_CYPHER_MODEL=gpt-3.5-turbo-1106\nHOSPITAL_QA_MODEL=gpt-3.5-turbo-0125\n\nCHATBOT_URL=http://host.docker.internal:8000/hospital-rag-agent\n</code></pre> <p>To complete your docker-compose.yml file, you\u2019ll need to add a chatbot_frontend service. Your final docker-compose.yml file should look like this:</p> <pre><code>version: '3'\n\nservices:\n  hospital_neo4j_etl:\n    build:\n      context: ./hospital_neo4j_etl\n    env_file:\n      - .env\n\n  chatbot_api:\n    build:\n      context: ./chatbot_api\n    env_file:\n      - .env\n    depends_on:\n      - hospital_neo4j_etl\n    ports:\n      - \"8000:8000\"\n\n  chatbot_frontend:\n    build:\n      context: ./chatbot_frontend\n    env_file:\n      - .env\n    depends_on:\n      - chatbot_api\n    ports:\n      - \"8501:8501\"\n</code></pre> <p>Finally, open a terminal and run:</p> <pre><code>docker-compose up --build\n</code></pre> <p>Once everything builds and runs, you can access the UI at http://localhost:8501/ and begin chatting with your chatbot:</p> <p></p> <p>You\u2019ve built a fully functioning hospital system chatbot end-to-end. Take some time to ask it questions, see the kinds of questions it\u2019s good at answering, find out where it fails, and think about how you might improve it with better prompting or data. You can start by making sure the example questions in the sidebar are answered successfully.</p>"},{"location":"langchain/agents/","title":"Agents","text":"<p>So far, you\u2019ve created a chain to answer questions using patient reviews. What if you want your chatbot to also answer questions about other hospital data, such as hospital wait times? Ideally, your chatbot can seamlessly switch between answering patient review and wait time questions depending on the user\u2019s query. To accomplish this, you\u2019ll need the following components:</p> <ol> <li>The patient review chain you already created</li> <li>A function that can look up wait times at a hospital</li> <li>A way for an LLM to know when it should answer questions about patient experiences or look up wait times</li> </ol> <p>To accomplish the third capability, you need an agent.</p> <p>An agent is a language model that decides on a sequence of actions to execute. Unlike chains where the sequence of actions is hard-coded, agents use a language model to determine which actions to take and in which order.</p> <p>Before building the agent, create the following function to generate fake wait times for a hospital:</p> <pre><code>import random\nimport time\n\ndef get_current_wait_time(hospital: str) -&gt; int | str:\n   \"\"\"Dummy function to generate fake wait times\"\"\"\n\n   if hospital not in [\"A\", \"B\", \"C\", \"D\"]:\n       return f\"Hospital {hospital} does not exist\"\n\n   # Simulate API call delay\n   time.sleep(1)\n\n   return random.randint(0, 10000)\n</code></pre> <p>In get_current_wait_time(), you pass in a hospital name, check if it\u2019s valid, and then generate a random number to simulate a wait time. In reality, this would be some sort of database query or API call, but this will serve the same purpose for this demonstration.</p> <p>You can now create an agent that decides between get_current_wait_time() and review_chain.invoke() depending on the question:</p> <pre><code>import dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import (\n    PromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n    ChatPromptTemplate,\n)\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.agents import (\n    create_openai_functions_agent,\n    Tool,\n    AgentExecutor,\n)\nfrom langchain import hub\nfrom langchain_intro.tools import get_current_wait_time\n\n# ...\n\ntools = [\n    Tool(\n        name=\"Reviews\",\n        func=review_chain.invoke,\n        description=\"\"\"Useful when you need to answer questions\n        about patient reviews or experiences at the hospital.\n        Not useful for answering questions about specific visit\n        details such as payer, billing, treatment, diagnosis,\n        chief complaint, hospital, or physician information.\n        Pass the entire question as input to the tool. For instance,\n        if the question is \"What do patients think about the triage system?\",\n        the input should be \"What do patients think about the triage system?\"\n        \"\"\",\n    ),\n    Tool(\n        name=\"Waits\",\n        func=get_current_wait_time,\n        description=\"\"\"Use when asked about current wait times\n        at a specific hospital. This tool can only get the current\n        wait time at a hospital and does not have any information about\n        aggregate or historical wait times. This tool returns wait times in\n        minutes. Do not pass the word \"hospital\" as input,\n        only the hospital name itself. For instance, if the question is\n        \"What is the wait time at hospital A?\", the input should be \"A\".\n        \"\"\",\n    ),\n]\n\nhospital_agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n\n\nhospital_agent = create_openai_functions_agent(\n    llm=chat_model,\n    prompt=hospital_agent_prompt,\n    tools=tools,\n)\n\nhospital_agent_executor = AgentExecutor(\n    agent=hospital_agent,\n    tools=tools,\n    return_intermediate_steps=True,\n    verbose=True,\n)\n</code></pre> <p>In this block, you import a few additional dependencies that you\u2019ll need to create the agent. You then define a list of Tool objects. A Tool is an interface that an agent uses to interact with a function. For instance, the first tool is named Reviews and it calls review_chain.invoke() if the question meets the criteria of description.</p> <p>Notice how description gives the agent instructions as to when it should call the tool. This is where good prompt engineering skills are paramount to ensuring the LLM calls the correct tool with the correct inputs.</p> <p>The second Tool in tools is named Waits, and it calls get_current_wait_time(). Again, the agent has to know when to use the Waits tool and what inputs to pass into it depending on the description.</p> <p>Next, you initialize a ChatOpenAI object using gpt-4-turbo as your language model. You then create an OpenAI functions agent with create_openai_functions_agent(). This creates an agent designed to pass inputs to functions. It does this by returning valid JSON objects that store function inputs and their corresponding value.</p> <p>To create the agent run time, you pass the agent and tools into AgentExecutor. Setting return_intermediate_steps and verbose to True will allow you to see the agent\u2019s thought process and the tools it calls.</p> <p>Start a new REPL session to give your new agent a spin:</p> <pre><code>from chatbot import hospital_agent_executor\n\nhospital_agent = hospital_agent_executor.invoke(\n    {\"input\": \"What is the current wait time at hospital C?\"}\n)\n\nhospital_agent_1 = hospital_agent_executor.invoke(\n    {\"input\": \"What have patients said about their comfort at the hospital?\"}\n)\nprint(hospital_agent)\nprint(hospital_agent_1)\n</code></pre> <p>You first import the agent and then call hospital_agent_executor.invoke() with a question about a wait time. As indicated in the output, the agent knows that you\u2019re asking about a wait time, and it passes C as input to the Waits tool. The Waits tool then calls get_current_wait_time(hospital=\"C\") and returns the corresponding wait time to the agent. The agent then uses this wait time to generate its final output.</p> <p>A similar process happens when you ask the agent about patient experience reviews, except this time the agent knows to call the Reviews tool with What have patients said about their comfort at the hospital? as input. The Reviews tool runs review_chain.invoke() using your full question as input, and the agent uses the response to generate its output.</p> <p>This is a profound capability. Agents give language models the ability to perform just about any task that you can write code for. Imagine all of the amazing, and potentially dangerous, chatbots you could build with agents.</p> <p>You now have all of the prerequisite LangChain knowledge needed to build a custom chatbot. Next up, you\u2019ll put on your AI engineer hat and learn about the business requirements and data needed to build your hospital system chatbot.</p> <p>All of the code you\u2019ve written so far was intended to teach you the fundamentals of LangChain, and it won\u2019t be included in your final chatbot. Feel free to start with an empty directory in Step 2, where you\u2019ll begin building your chatbot.</p>"},{"location":"langchain/chat_model/","title":"Chat Models","text":"<p>You might\u2019ve guessed that the core component of LangChain is the LLM. LangChain provides a modular interface for working with LLM providers such as OpenAI, Cohere, HuggingFace, Anthropic, Together AI, and others. In most cases, all you need is an API key from the LLM provider to get started using the LLM with LangChain. LangChain also supports LLMs or other language models hosted on your own machine.</p> <p>You\u2019ll use OpenAI for this tutorial, but keep in mind there are many great open- and closed-source providers out there. You can always test out different providers and optimize depending on your application\u2019s needs and cost constraints.Before moving forward, make sure you\u2019re signed up for an Azure account and you have a valid azure API keys.</p> <pre><code>AZURE_OPENAI_API_KEY=\"&lt;YOUR-AZURE_API_KEY&gt;\"\nAZURE_OPENAI_ENDPOINT=\"YOUR-AZURE_API_BASE\"\nOPENAI_API_VERSION=\"YOUR-AZURE_API_VERSION\"\n</code></pre> <p>While you can interact directly with LLM objects in LangChain, a more common abstraction is the chat model. Chat models use LLMs under the hood, but they\u2019re designed for conversations, and they interface with chat messages rather than raw text.</p> <p>Using chat messages, you provide an LLM with additional detail about the kind of message you\u2019re sending. All messages have role and content properties. The role tells the LLM who is sending the message, and the content is the message itself. Here are the most commonly used messages:</p> <p>HumanMessage: A message from the user interacting with the language model. AIMessage: A message from the language model. SystemMessage: A message that tells the language model how to behave. Not all providers support the SystemMessage. There are other messages types, like FunctionMessage and ToolMessage, but you\u2019ll learn more about those when you build an agent.</p> <p>Getting started with chat models in LangChain is straightforward. To instantiate an OpenAI chat model, navigate to langchain_intro and add the following code to chatbot.py:</p> <pre><code>import dotenv\nfrom langchain_openai import AzureChatOpenAI\n\ndotenv.load_dotenv()\n# Create an instance of Azure OpenAI\n# Replace the deployment name with your own\nllm = AzureChatOpenAI(\n    deployment_name=\"gpt-4-turbo\",\n)\nllm_response = llm.invoke(\"Hello, how are you?\")\nprint(llm_response)\n</code></pre> <p>You first import dotenv and ChatOpenAI. Then you call dotenv.load_dotenv() which reads and stores environment variables from .env. By default, dotenv.load_dotenv() assumes .env is located in the current working directory, but you can pass the path to other directories if .env is located elsewhere.</p> <p>You then instantiate a AzureChatOpenAI model using GPT 4 Turbo as the base LLM, and default temperature to 1 Azure OpenAI offers a diversity of models with varying price points, capabilities, and performances.</p> <p>To use chat_model, open the project directory, start a Python interpreter, and run the following code:</p> <pre><code>from langchain.schema.messages import HumanMessage, SystemMessage\nfrom chatbot import chat_model\n\nmessages = [\n    SystemMessage(\n        content=\"\"\"You're an assistant knowledgeable about\n        healthcare. Only answer healthcare-related questions.\"\"\"\n    ),\n    HumanMessage(content=\"What is Medicaid managed care?\"),\n]\nllm_message_res = chat_model(messages)\nprint(llm_message_res)\n</code></pre> <p>In this block, you import HumanMessage and SystemMessage, as well as your chat model. You then define a list with a SystemMessage and a HumanMessage and run them through chat_model with chat_model(message). Under the hood, chat_model makes a request to an OpenAI endpoint serving gpt-4, and the results are returned as an AIMessage.</p> <p>As you can see, the chat model answered What is Medicaid managed care? provided in the HumanMessage. You might be wondering what the chat model did with the SystemMessage in this context. Notice what happens when you ask the following question:</p> <pre><code>messages = [\n    SystemMessage(\n        content=\"\"\"You're an assistant knowledgeable about\n        healthcare. Only answer healthcare-related questions.\"\"\"\n    ),\n    HumanMessage(content=\"How do I change a tire?\"),\n]\nllm_message_res = chat_model(messages)\nprint(llm_message_res)\n</code></pre> <p>As described earlier, the SystemMessage tells the model how to behave. In this case, you told the model to only answer healthcare-related questions. This is why it refuses to tell you how to change your tire. The ability to control how an LLM relates to the user through text instructions is powerful, and this is the foundation for creating customized chatbots through prompt engineering.</p> <p>While chat messages are a nice abstraction and are good for ensuring that you\u2019re giving the LLM the right kind of message, you can also pass raw strings into chat models:</p> <p>While chat messages are a nice abstraction and are good for ensuring that you\u2019re giving the LLM the right kind of message, you can also pass raw strings into chat models:</p> <pre><code>&gt;&gt;&gt; chat_model.invoke(\"What is blood pressure?\")\nAIMessage(content='Blood pressure is the force exerted by\nthe blood against the walls of the blood vessels, particularly\nthe arteries, as it is pumped by the heart. It is measured in\nmillimeters of mercury (mmHg) and is typically expressed as two\nnumbers: systolic pressure over diastolic pressure. The systolic\npressure represents the force when the heart contracts and pumps\nblood into the arteries, while the diastolic pressure represents\nthe force when the heart is at rest between beats. Blood pressure\nis an important indicator of cardiovascular health and can be influenced\nby various factors such as age, genetics, lifestyle, and underlying medical\nconditions.')\n</code></pre> <p>In this code block, you pass the string What is blood pressure? directly to chat_model.invoke(). If you want to control the LLM\u2019s behavior without a SystemMessage here, you can include instructions in the string input.</p> <p>NOTE: In these examples, you used .invoke(), but LangChain has other methods that interact with LLMs. For instance, .stream() returns the response one token at time, and .batch() accepts a list of messages that the LLM responds to in one call.</p> <p>Each method also has an analogous asynchronous method. For instance, you can run .invoke() asynchronously with ainvoke().</p> <p>Next up, you\u2019ll learn a modular way to guide your model\u2019s response, as you did with the SystemMessage, making it easier to customize your chatbot.</p>"},{"location":"langchain/introduction/","title":"Get Familiar With LangChain","text":"<p>Before you design and develop your chatbot, you need to know how to use LangChain. In this section, you\u2019ll get to know LangChain\u2019s main components and features by building a preliminary version of your hospital system chatbot. This will give you all the necessary tools to build your full chatbot.</p> <p>Use your favorite code editor to create a new Python project, and be sure to create a virtual environment for its dependencies. Make sure you have Python 3.10 or later installed. Activate your virtual environment and install the following libraries:</p> <pre><code>(venv) $ python -m pip install langchain==0.1.0 openai==1.7.2 langchain-openai==0.0.2 langchain-community==0.0.12 langchainhub==0.1.14\n</code></pre> <p>You\u2019ll also want to install python-dotenv to help you manage environment variables:</p> <pre><code>(venv) $ python -m pip install python-dotenv\n</code></pre> <p>Python-dotenv loads environment variables from .env files into your Python environment, and you\u2019ll find this handy as you develop your chatbot. However, you\u2019ll eventually deploy your chatbot with Docker, which can handle environment variables for you, and you won\u2019t need Python-dotenv anymore.</p> <p>If you haven\u2019t already, you\u2019ll need to download reviews.csv from the materials or GitHub repo for this tutorial:</p> <p>Next, open the project directory and add the following folders and files:</p> <p>langchain</p>"},{"location":"langchain/lcel/","title":"Chains and LangChain Expression Language (LCEL)","text":"<p>The glue that connects chat models, prompts, and other objects in LangChain is the chain. A chain is nothing more than a sequence of calls between objects in LangChain. The recommended way to build chains is to use the LangChain Expression Language (LCEL).</p> <p>To see how this works, take a look at how you\u2019d create a chain with a chat model and prompt template:</p> <pre><code>import dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import (\n    PromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n    ChatPromptTemplate,\n)\n\ndotenv.load_dotenv()\n\nreview_template_str = \"\"\"Your job is to use patient\nreviews to answer questions about their experience at\na hospital. Use the following context to answer questions.\nBe as detailed as possible, but don't make up any information\nthat's not from the context. If you don't know an answer, say\nyou don't know.\n\n{context}\n\"\"\"\n\nreview_system_prompt = SystemMessagePromptTemplate(\n    prompt=PromptTemplate(\n        input_variables=[\"context\"],\n        template=review_template_str,\n    )\n)\n\nreview_human_prompt = HumanMessagePromptTemplate(\n    prompt=PromptTemplate(\n        input_variables=[\"question\"],\n        template=\"{question}\",\n    )\n)\nmessages = [review_system_prompt, review_human_prompt]\n\nreview_prompt_template = ChatPromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    messages=messages,\n)\n\nchat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n\nreview_chain = review_prompt_template | chat_model\n</code></pre> <p>Lines 1 to 42 are what you\u2019ve already done. Namely, you define review_prompt_template which is a prompt template for answering questions about patient reviews, and you instantiate a gpt-4-turbo chat model. In line 44, you define review_chain with the | symbol, which is used to chain review_prompt_template and chat_model together.</p> <p>This creates an object, review_chain, that can pass questions through review_prompt_template and chat_model in a single function call. In essence, this abstracts away all of the internal details of review_chain, allowing you to interact with the chain as if it were a chat model.</p> <p>After saving the updated chatbot.py, start a new REPL session in your base project folder. Here\u2019s how you can use review_chain:</p> <pre><code>from langchain_intro.chatbot import review_chain\n\ncontext = \"I had a great stay!\"\nquestion = \"Did anyone have a positive experience?\"\n\nreview_chain.invoke({\"context\": context, \"question\": question})\n</code></pre> <p>In this block, you import review_chain and define context and question as before. You then pass a dictionary with the keys context and question into review_chan.invoke(). This passes context and question through the prompt template and chat model to generate an answer.</p> <p>In general, the LCEL allows you to create arbitrary-length chains with the pipe symbol (|). For instance, if you wanted to format the model\u2019s response, then you could add an output parser to the chain:</p> <pre><code>import dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import (\n    PromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n    ChatPromptTemplate,\n)\nfrom langchain_core.output_parsers import StrOutputParser\n\n# ...\n\noutput_parser = StrOutputParser()\n\nreview_chain = review_prompt_template | chat_model | output_parser\n</code></pre> <p>Here, you add a StrOutputParser() instance to review_chain, which will make the model\u2019s response more readable. Start a new REPL session and give it a try:</p> <pre><code>&gt;&gt;&gt; from langchain_intro.chatbot import review_chain\n\n&gt;&gt;&gt; context = \"I had a great stay!\"\n&gt;&gt;&gt; question = \"Did anyone have a positive experience?\"\n\n&gt;&gt;&gt; review_chain.invoke({\"context\": context, \"question\": question})\n'Yes, the patient had a great stay and had a\npositive experience at the hospital.'\n</code></pre> <p>This block is the same as before, except now you can see that review_chain returns a nicely-formatted string rather than an AIMessage.</p> <p>The power of chains is in the creativity and flexibility they afford you. You can chain together complex pipelines to create your chatbot, and you end up with an object that executes your pipeline in a single method call. Next up, you\u2019ll layer another object into review_chain to retrieve documents from a vector database.</p>"},{"location":"langchain/prompt_template/","title":"Prompt Templates","text":"<p>LangChain allows you to design modular prompts for your chatbot with prompt templates. Quoting LangChain\u2019s documentation, you can think of prompt templates as predefined recipes for generating prompts for language models.</p> <p>Suppose you want to build a chatbot that answers questions about patient experiences from their reviews. Here\u2019s what a prompt template might look like for this:</p> <pre><code>from langchain.prompts import ChatPromptTemplate\n\nreview_template_str = \"\"\"Your job is to use patient\nreviews to answer questions about their experience at a hospital.\nUse the following context to answer questions. Be as detailed\nas possible, but don't make up any information that's not\nfrom the context. If you don't know an answer, say you don't know.\n\n{context}\n\n{question}\n\"\"\"\n\nreview_template = ChatPromptTemplate.from_template(review_template_str)\n\ncontext = \"I had a great stay!\"\nquestion = \"Did anyone have a positive experience?\"\n\nreview_template.format(context=context, question=question)\n</code></pre> <p>You first import ChatPromptTemplate and define review_template_str, which contains the instructions that you\u2019ll pass to the model, along with the variables context and question in replacement fields that LangChain delimits with curly braces ({}). You then create a ChatPromptTemplate object from review_template_str using the class method .from_template().</p> <p>With review_template instantiated, you can pass context and question into the string template with review_template.format(). The results may look like you\u2019ve done nothing more than standard Python string interpolation, but prompt templates have a lot of useful features that allow them to integrate with chat models.</p> <p>Notice how your previous call to review_template.format() generated a string with Human at the beginning. This is because ChatPromptTemplate.from_template() assumes the string template is a human message by default. To change this, you can create more detailed prompt templates for each chat message that you want the model to process:</p> <pre><code>from langchain.prompts import (\n    PromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n    ChatPromptTemplate,\n)\n\nreview_system_template_str = \"\"\"Your job is to use patient\nreviews to answer questions about their experience at a\nhospital. Use the following context to answer questions.\nBe as detailed as possible, but don't make up any information\nthat's not from the context. If you don't know an answer, say\nyou don't know.\n\n{context}\n\"\"\"\n\nreview_system_prompt = SystemMessagePromptTemplate(\n    prompt=PromptTemplate(\n        input_variables=[\"context\"], template=review_system_template_str\n    )\n)\n\nreview_human_prompt = HumanMessagePromptTemplate(\n    prompt=PromptTemplate(\n        input_variables=[\"question\"], template=\"{question}\"\n    )\n)\n\nmessages = [review_system_prompt, review_human_prompt]\nreview_prompt_template = ChatPromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    messages=messages,\n)\ncontext = \"I had a great stay!\"\nquestion = \"Did anyone have a positive experience?\"\n\nreview_prompt_template.format_messages(context=context, question=question)\n</code></pre> <p>In this block, you import separate prompt templates for HumanMessage and SystemMessage. You then define a string, review_system_template_str, which serves as the template for a SystemMessage. Notice how you only declare a context variable in review_system_template_str.</p> <p>From this, you create review_system_prompt which is a prompt template specifically for SystemMessage. Next you create a review_human_prompt for the HumanMessage. Notice how the template parameter is just a string with the question variable.</p> <p>You then add review_system_prompt and review_human_prompt to a list called messages and create review_prompt_template, which is the final object that encompasses the prompt templates for both the SystemMessage and HumanMessage. Calling review_prompt_template.format_messages(context=context, question=question) generates a list with a SystemMessage and HumanMessage, which can be passed to a chat model.</p> <p>To see how to combine chat models and prompt templates, you\u2019ll build a chain with the LangChain Expression Language (LCEL). This helps you unlock LangChain\u2019s core functionality of building modular customized interfaces over chat models.</p>"},{"location":"langchain/retrieval_objects/","title":"Retrieval Objects","text":"<p>The goal of review_chain is to answer questions about patient experiences in the hospital from their reviews. So far, you\u2019ve manually passed reviews in as context for the question. While this can work for a small number of reviews, it doesn\u2019t scale well. Moreover, even if you can fit all reviews into the model\u2019s context window, there\u2019s no guarantee it will use the correct reviews when answering a question.</p> <p>To overcome this, you need a retriever. The process of retrieving relevant documents and passing them to a language model to answer questions is known as retrieval-augmented generation (RAG).</p> <p>For this example, you\u2019ll store all the reviews in a vector database called ChromaDB. If you\u2019re unfamiliar with this database tool and topics, then check out Embeddings and Vector Databases with ChromaDB before continuing.</p> <pre><code>poetry add chromadb\n</code></pre> <p>With this installed, you can use the following code to create a ChromaDB vector database with patient reviews:</p> <pre><code>import dotenv\nfrom langchain.document_loaders.csv_loader import CSVLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\nREVIEWS_CSV_PATH = \"data/reviews.csv\"\nREVIEWS_CHROMA_PATH = \"chroma_data\"\n\ndotenv.load_dotenv()\n\nloader = CSVLoader(file_path=REVIEWS_CSV_PATH, source_column=\"review\")\nreviews = loader.load()\n\nreviews_vector_db = Chroma.from_documents(\n    reviews, OpenAIEmbeddings(), persist_directory=REVIEWS_CHROMA_PATH\n)\n</code></pre> <p>In lines 2 to 4, you import the dependencies needed to create the vector database. You then define REVIEWS_CSV_PATH and REVIEWS_CHROMA_PATH, which are paths where the raw reviews data is stored and where the vector database will store data, respectively.</p> <p>You\u2019ll get an overview of the hospital system data later, but all you need to know for now is that reviews.csv stores patient reviews. The review column in reviews.csv is a string with the patient\u2019s review.</p> <p>In lines 11 and 12, you load the reviews using LangChain\u2019s CSVLoader. In lines 14 to 16, you create a ChromaDB instance from reviews using the default Azure OpenAI embedding model, and you store the review embeddings at REVIEWS_CHROMA_PATH.</p> <p>Note: In practice, if you\u2019re embedding a large document, you should use a text splitter. Text splitters break the document into smaller chunks before running them through an embedding model. This is important because embedding models have a fixed-size context window, and as the size of the text grows, an embedding\u2019s ability to accurately represent the text decreases.</p> <p>For this example, you can embed each review individually because they\u2019re relatively small.</p> <pre><code>(venv) $ python langchain_intro/create_retriever.py\n</code></pre> <p>It should only take a minute or so to run, and afterwards you can start performing semantic search over the review embeddings:</p> <pre><code>import dotenv\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nREVIEWS_CHROMA_PATH = \"chroma_data/\"\ndotenv.load_dotenv()\nembd = AzureOpenAIEmbeddings(\n    azure_deployment=\"text-embedding-ada-002\",\n    openai_api_version=\"2023-05-15\",\n)\n\nreviews_vector_db = Chroma(\n    persist_directory=REVIEWS_CHROMA_PATH,\n    embedding_function=embd,\n)\n\nquestion = \"\"\"Has anyone complained about\n           communication with the hospital staff?\"\"\"\nrelevant_docs = reviews_vector_db.similarity_search(question, k=3)\n\nprint(relevant_docs[0].page_content)\nprint(relevant_docs[1].page_content)\nprint(relevant_docs[2].page_content)\n</code></pre> <p>You import the dependencies needed to call ChromaDB and specify the path to the stored ChromaDB data in REVIEWS_CHROMA_PATH. You then load environment variables using dotenv.load_dotenv() and create a new Chroma instance pointing to your vector database. Notice how you have to specify an embedding function again when connecting to your vector database. Be sure this is the same embedding function that you used to create the embeddings.</p> <p>Next, you define a question and call .similarity_search() on reviews_vector_db, passing in question and k=3. This creates an embedding for the question and searches the vector database for the three most similar review embeddings to question embedding. In this case, you see three reviews where patients complained about communication, which is exactly what you asked for!</p> <p>The last thing to do is add your reviews retriever to review_chain so that relevant reviews are passed to the prompt as context. Here\u2019s how you do that:</p> <pre><code>import dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import (\n    PromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n    ChatPromptTemplate,\n)\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.schema.runnable import RunnablePassthrough\n\nREVIEWS_CHROMA_PATH = \"chroma_data/\"\n\n# ...\n\nreviews_vector_db = Chroma(\n    persist_directory=REVIEWS_CHROMA_PATH,\n    embedding_function=OpenAIEmbeddings()\n)\n\nreviews_retriever  = reviews_vector_db.as_retriever(k=10)\n\nreview_chain = (\n    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n    | review_prompt_template\n    | chat_model\n    | StrOutputParser()\n)\n</code></pre> <p>As before, you import ChromaDB\u2019s dependencies, specify the path to your ChromaDB data, and instantiate a new Chroma object. You then create reviews_retriever by calling .as_retriever() on reviews_vector_db to create a retriever object that you\u2019ll add to review_chain. Because you specified k=10, the retriever will fetch the ten reviews most similar to the user\u2019s question.</p> <p>You then add a dictionary with context and question keys to the front of review_chain. Instead of passing context in manually, review_chain will pass your question to the retriever to pull relevant reviews. Assigning question to a RunnablePassthrough object ensures the question gets passed unchanged to the next step in the chain.</p> <p>You now have a fully functioning chain that can answer questions about patient experiences from their reviews. Start a new REPL session and try it out:</p> <pre><code>from chatbot import review_chain\n\ncontext = \"I had a great stay!\"\n# question = \"Did anyone have a positive experience?\"\n\n# llm_response = review_chain.invoke({\"context\": context, \"question\": question})\n# print(llm_response)\nquestion = \"\"\"Has anyone complained about\n           communication with the hospital staff?\"\"\"\nreview_chain_result = review_chain.invoke(question)\nprint(review_chain_result)\n</code></pre> <p>As you can see, you only call review_chain.invoke(question) to get retrieval-augmented answers about patient experiences from their reviews. You\u2019ll improve upon this chain later by storing review embeddings, along with other metadata, in Neo4j.</p> <p>Now that you understand chat models, prompts, chains, and retrieval, you\u2019re ready to dive into the last LangChain concept\u2014agents.</p>"},{"location":"neo4j/design_graphdb/","title":"Design the Hospital System Graph Database","text":"<p>Now that you have a running Neo4j AuraDB instance, you need to decide which nodes, relationships, and properties you want to store. One of the most popular ways to represent this is with a flowchart. Based on your understanding of the hospital system data, you come up with the following design:</p> <p></p> <p>This diagram shows you all of the nodes and relationships in the hospital system data. One useful way to think about this flowchart is to start with the Patient node and follow the relationships. A Patient has a visit at a hospital, and the hospital employs a physician to treat the visit which is covered by an insurance payer.</p> <p>Here are the properties stored in each node:</p> <p></p> <p>The majority of these properties come directly from the fields you explored in step 2. One notable difference is that Review nodes have an embedding property, which is a vector representation of the patient_name, physician_name, and text properties. This allows you to do vector searches over review nodes like you did with ChromaDB.</p> <p>Here are the relationship properties:</p> <p></p> <p>As you can see, COVERED_BY is the only relationship with more than an id property. The service_date is the date the patient was discharged from a visit, and billing_amount is the amount charged to the payer for the visit.</p> <p>Note: This fake hospital system data has a relatively small number of nodes and relationships than what you\u2019d typically see in an enterprise setting. However, you can easily imagine how many more nodes and relationships you could add for a real hospital system. For instance, nurses, pharmacists, pharmacies, prescription drugs, surgeries, patient relatives, and many more hospital entities could be represented as nodes.</p> <p>You could also redesign this so that diagnoses and symptoms are represented as nodes instead of properties, or you could add more relationship properties. You could do all of this without changing the design you already have. This is the beauty of graphs\u2014you simply add more nodes and relationships as your data evolves.</p> <p>Now that you have an overview of the hospital system design you\u2019ll use, it\u2019s time to move your data into Neo4j!</p>"},{"location":"neo4j/introduction/","title":"Set Up a Neo4j Graph Database","text":"<p>As you saw in step 2 your hospital system data is currently stored in CSV files. Before building your chatbot, you need to store this data in a database that your chatbot can query. You\u2019ll use Neo4j AuraDB for this.</p> <p>Before learning how to set up a Neo4j AuraDB instance, you\u2019ll get an overview of graph databases, and you\u2019ll see why using a graph database may be a better choice than a relational database for this project.</p>"},{"location":"neo4j/overview/","title":"A Brief Overview of Graph Databases","text":"<p>Graph databases, such as Neo4j, are databases designed to represent and process data stored as a graph. Graph data consists of nodes, edges or relationships, and properties. Nodes represent entities, relationships connect entities, and properties provide additional metadata about nodes and relationships.</p> <p>For example, here\u2019s how you might represent hospital system nodes and relationships in a graph:</p> <p></p> <p>This graph has three nodes - Patient, Visit, and Payer. Patient and Visit are connected by the HAS relationship, indicating that a hospital patient has a visit. Similarly, Visit and Payer are connected by the COVERED_BY relationship, indicating that an insurance payer covers a hospital visit.</p> <p>Notice how the relationships are represented by an arrow indicating their direction. For example, the direction of the HAS relationship tells you that a patient can have a visit, but a visit cannot have a patient.</p> <p>Both nodes and relationships can have properties. In this example, Patient nodes have id, name, and date of birth properties, and the COVERED_BY relationship has service date and billing amount properties. Storing data in a graph like this has several advantages:</p> <ul> <li> <p>Simplicity: Modeling real-world relationships between entities is natural in graph databases, reducing the need for complex schemas that require multiple join operations to answer queries.</p> </li> <li> <p>Relationships: Graph databases excel at handling complex relationships. Traversing relationships is efficient, making it easy to query and analyze connected data.</p> </li> <li> <p>Flexibility: Graph databases are schema-less, allowing for easy adaptation to changing data structures. This flexibility is beneficial for evolving data models.</p> </li> <li> <p>Performance: Retrieving connected data is faster in graph databases than in relational databases, especially for scenarios involving complex queries with multiple relationships.</p> </li> <li> <p>Pattern Matching: Graph databases support powerful pattern-matching queries, making it easier to express and find specific structures within the data.</p> </li> </ul> <p>When you have data with many complex relationships, the simplicity and flexibility of graph databases makes them easier to design and query compared to relational databases. As you\u2019ll see later, specifying relationships in graph database queries is concise and doesn\u2019t involve complicated joins. If you\u2019re interested, Neo4j illustrates this well with a realistic example database in their documentation.</p> <p>Because of this concise data representation, there\u2019s less room for error when an LLM generates graph database queries. This is because you only need to tell the LLM about the nodes, relationships, and properties in your graph database. Contrast this with relational databases where the LLM must navigate and retain knowledge of the table schemas and foreign key relationships throughout your database, leaving more room for error in SQL generation.</p> <p>Next, you\u2019ll begin working with graph databases by setting up a Neo4j AuraDB instance. After that, you\u2019ll move the hospital system into your Neo4j instance and learn how to query it.</p>"},{"location":"neo4j/query/","title":"Query the Hospital System Graph","text":"<p>The last thing you need to do before building your chatbot is get familiar with Cypher syntax. Cypher is Neo4j\u2019s query language, and it\u2019s fairly intuitive to learn, especially if you\u2019re familiar with SQL. This section will cover the basics, and that\u2019s all you need to build the chatbot. You can check out Neo4j\u2019s documentation for a more comprehensive Cypher overview.</p> <p>The most commonly used key word for reading data in Cypher is MATCH, and it\u2019s used to specify patterns to look for in the graph. The simplest pattern is one with a single node. For example, if you wanted to find the first five patient nodes written to the graph, you could run the following Cypher query:</p> <pre><code>MATCH (p:Patient)\nRETURN p LIMIT 5;\n</code></pre> <p></p> <p>In this query, you\u2019re matching on Patient nodes. In Cypher, nodes are always indicated by parentheses. The p in (p:Patient) is an alias that you can reference later in the query. RETURN p LIMIT 5; tells Neo4j to only return five patient nodes. You can run this query in the Neo4j UI, and the results should look like this:</p> <p>The Table view shows you the five Patient nodes returned along with their properties. You can also explore the graph and raw view if you\u2019re interested.</p> <p>While matching on a single node is straightforward, sometimes that\u2019s all you need to get useful insights. For example, if your stakeholder said give me a summary of visit 56, the following query gives you the answer:</p> <pre><code>MATCH (v:Visit)\nWHERE v.id = 56\nRETURN v;\n</code></pre> <p>This query matches Visit nodes that have an id of 56, specified by WHERE v.id = 56. You can filter on arbitrary node and relationship properties in WHERE clauses. The results of this query look like this:</p> <p></p> <p>From the query output, you can see the returned Visit indeed has id 56. You could then look at all of the visit properties to come up with a verbal summary of the visit\u2014this is what your Cypher chain will do.</p> <p>Matching on nodes is great, but the real power of Cypher comes from its ability to match on relationship patterns. This gives you insight into sophisticated relationships, exploiting the power of graph databases. Continuing with the Visit query, you probably want to know which Patient the Visit belongs to. You can get this from the HAS relationship:</p> <pre><code>MATCH (p:Patient)-[h:HAS]-&gt;(v:Visit)\nWHERE v.id = 56\nRETURN v,h,p;\n</code></pre> <p>This Cypher query searches for the Patient that has a Visit with id 56. You\u2019ll notice that the relationship HAS is surrounded by square brackets instead of parentheses, and its directionality is indicated by an arrow. If you tried MATCH (p:Patient)&lt;-[h:HAS]-(v:Visit), the query would return nothing because the direction of HAS relationship is incorrect.</p> <p>The query results look like this:</p> <p></p> <p>Notice the output includes data for the Visit, HAS relationship, and Patient. This gives you more insight than if you only match on Visit nodes. If you wanted to see which physicians treated the patient during the Visit, you could add the following relationship to the query:</p> <pre><code>MATCH (p:Patient)-[h:HAS]-&gt;(v:Visit)&lt;-[t:TREATS]-(ph:Physician)\nWHERE v.id = 56\nRETURN v,p,ph\n</code></pre> <p>This statement (p:Patient)-[h:HAS]-&gt;(v:Visit)&lt;-[t:TREATS]-(ph:Physician) tells Neo4j to find all patterns where a Patient has a Visit that\u2019s treated by a Physician. If you wanted to match all relationships going in and out of the Visit node, you could run this query:</p> <pre><code>MATCH (v:Visit)-[r]-(n)\nWHERE v.id = 56\nRETURN r,n;\n</code></pre> <p>Notice now that the relationship [r], has no direction with respect to (v:Visit) or (n). In essence, this match statement will look for all relationships that go in and out of Visit 56, along with the nodes connected to those relationships. Here\u2019s the results:</p> <p></p> <p>This gives you a nice view of all the relationships and nodes associated with Visit 56. Think about how powerful this representation is. Instead of performing multiple SQL joins, as you\u2019d have to do in a relational database, you get all of the information about how a Visit is connected to the entire hospital system with three short lines of Cypher.</p> <p>You can imagine how much more powerful this would become as more nodes and relationships are added to the graph database. For example, you could record which nurses, pharmacies, drugs, or surgeries are associated with the Visit. Each relationship that you add would necessitate another join in SQL, but the above Cypher query about Visit 56 would remain unchanged.</p> <p>The last thing you\u2019ll cover in this section is how to perform aggregations in Cypher. So far, you\u2019ve only queried raw data from nodes and relationships, but you can also compute aggregate statistics in Cypher.</p> <p>Suppose you wanted to answer the question What is the total number of visits and total billing amount for visits covered by Aetna in Texas? Here\u2019s the Cypher query that would answer this question:</p> <pre><code>MATCH (p:Payer)&lt;-[c:COVERED_BY]-(v:Visit)-[:AT]-&gt;(h:Hospital)\nWHERE p.name = \"Aetna\"\nAND h.state_name = \"TX\"\nRETURN COUNT(*) as num_visits,\nSUM(c.billing_amount) as total_billing_amount;\n</code></pre> <p>In this query, you first match all Visits that occur at a Hospital and are covered by a Payer. You then filter to Payers with a name property of Aetna and Hospitals with a state_name of TX. Lastly, COUNT(*) counts the number of matched patterns, and SUM(c.billing_amount) gives you the total billing amount. The output looks like this:</p> <p></p> <p>The results tell you there were 198 Visits matching this pattern with a total billing amount of about $5,056,439.</p> <p>You now have a solid understanding of Cypher fundamentals, as well as the kinds of questions you can answer. In short, Cypher is great at matching complicated relationships without requiring a verbose query. There\u2019s a lot more that you can do with Neo4j and Cypher, but the knowledge you obtained in this section is enough to start building the chatbot, and that\u2019s what you\u2019ll do next.</p>"},{"location":"neo4j/setup/","title":"Create a Neo4j Account and AuraDB Instance","text":"<p>To get started using Neo4j, you can create a free Neo4j AuraDB account. The landing page should look something like this:</p> <p></p> <p>Click the Start Free button and create an account. Once you\u2019re signed in, you should see the Neo4j Aura console:</p> <p></p> <p>Click New Instance and create a free instance. A modal should pop up similar to this:</p> <p></p> <p>After you click Download and Continue, your instance should be created and a text file containing the Neo4j database credentials should download. Once the instance is created, you\u2019ll see its status is Running. There should be no nodes or relationships yet:</p> <p></p> <p>Next, open the text file you downloaded with your Neo4j credentials and copy the NEO4J_URI, NEO4J_USERNAME, and NEO4J_PASSWORD into your .env file:</p> <pre><code>NEO4J_URI=&lt;YOUR_NEO4J_URI&gt;\nNEO4J_USERNAME=&lt;YOUR_NEO4J_URI&gt;\nNEO4J_PASSWORD=&lt;YOUR_NEO4J_PASSWORD&gt;\n</code></pre> <p>You\u2019ll use these environment variables to connect to your Neo4j instance in Python so that your chatbot can execute queries.</p> <p>Note: By default, your NEO4J_URI should be similar to neo4j+s://.databases.neo4j.io. The URL scheme neo4j+s uses CA-signed certificates only, which might not work for you. If this is the case, change your URI to use the neo4j+ssc URL scheme - neo4j+ssc://.databases.neo4j.io. You can read more about what this means in the Neo4j documentation on connection protocols and security.</p> <p>You now have everything in place to interact with your Neo4j instance. Next up, you\u2019ll design the hospital system graph database. This will tell you how the hospital entities are related, and it will inform the kinds of queries you can run.</p>"},{"location":"neo4j/upload_data/","title":"Upload Data to Neo4j","text":"<p>With a running Neo4j instance and an understanding of the nodes, properties, and relationships you want to store, you can move the hospital system data into Neo4j. For this, you\u2019ll create a folder called hospital_neo4j_etl with a few empty files. You\u2019ll also want to create a docker-compose.yml file in your project\u2019s root directory:</p> <pre><code>./\n\u2502\n\u251c\u2500\u2500 hospital_neo4j_etl/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2502   \u2514\u2500\u2500 hospital_bulk_csv_write.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 docker-compose.yml\n</code></pre> <p>Your .env file should have the following environment variables:</p> <pre><code>NEO4J_URI=&lt;YOUR_NEO4J_URI&gt;\nNEO4J_USERNAME=&lt;YOUR_NEO4J_URI&gt;\nNEO4J_PASSWORD=&lt;YOUR_NEO4J_PASSWORD&gt;\n\nHOSPITALS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/hospitals.csv\nPAYERS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/payers.csv\nPHYSICIANS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/physicians.csv\nPATIENTS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/patients.csv\nVISITS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/visits.csv\nREVIEWS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/reviews.csv\n</code></pre> <p>Notice that you\u2019ve stored all of the CSV files in a public location on GitHub. Because your Neo4j AuraDB instance is running in the cloud, it can\u2019t access files on your local machine, and you have to use HTTP or upload the files directly to your instance. For this example, you can either use the link above, or upload the data to another location.</p> <p>Note: If you\u2019re uploading proprietary data to Neo4j, always ensure that it\u2019s stored in a secure location and transferred appropriately. The data used for this project is all synthetic and not proprietary, so there\u2019s no problem with uploading it over a public HTTP connection. However, this would not be a good idea in practice. You can read more about secure ways to import data into Neo4j in their documentation.</p> <p>Once you have your .env file populated, open pyproject.toml, which provides configuration, metadata, and dependencies defined in the TOML format:</p> <pre><code>[project]\ndependencies = [\n    ....\n   \"neo4j==5.14.1\",\n   \"retry==0.9.2\"\n]\n\n\n[tool.poetry.group.dev.dependencies]\n....\nblack = \"^24.3.0\"\nflake8 = \"^7.0.0\"\n</code></pre> <p>This project is a bare bones extract, transform, load (ETL) process that moves data into Neo4j, so it\u2019s only dependencies are neo4j and retry. The main script for the ETL is hospital_neo4j_etl/src/hospital_bulk_csv_write.py. It\u2019s too long to include the full script here, but you\u2019ll get a feel for the main steps hospital_neo4j_etl/src/hospital_bulk_csv_write.py executes. You can copy the full script from the materials:</p> <p>First, you import dependencies, load environment variables, and configure logging:</p> <pre><code>import os\nimport logging\nfrom retry import retry\nfrom neo4j import GraphDatabase\n\nHOSPITALS_CSV_PATH = os.getenv(\"HOSPITALS_CSV_PATH\")\nPAYERS_CSV_PATH = os.getenv(\"PAYERS_CSV_PATH\")\nPHYSICIANS_CSV_PATH = os.getenv(\"PHYSICIANS_CSV_PATH\")\nPATIENTS_CSV_PATH = os.getenv(\"PATIENTS_CSV_PATH\")\nVISITS_CSV_PATH = os.getenv(\"VISITS_CSV_PATH\")\nREVIEWS_CSV_PATH = os.getenv(\"REVIEWS_CSV_PATH\")\n\nNEO4J_URI = os.getenv(\"NEO4J_URI\")\nNEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\nNEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s]: %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\n\nLOGGER = logging.getLogger(__name__)\n\n# ...\n</code></pre> <p>You import the GraphDatabase class from neo4j to connect to your running instance. Notice here that you\u2019re no longer using Python-dotenv to load environment variables. Instead, you\u2019ll pass environment variables into the Docker container that runs your script. Next, you\u2019ll define functions to move hospital data into Neo4j following your design:</p> <pre><code># ...\n\nNODES = [\"Hospital\", \"Payer\", \"Physician\", \"Patient\", \"Visit\", \"Review\"]\n\ndef _set_uniqueness_constraints(tx, node):\nquery = f\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (n:{node})\nREQUIRE n.id IS UNIQUE;\"\"\"\n_ = tx.run(query, {})\n\n@retry(tries=100, delay=10)\ndef load_hospital_graph_from_csv() -&gt; None:\n\"\"\"Load structured hospital CSV data following\na specific ontology into Neo4j\"\"\"\n\n    driver = GraphDatabase.driver(\n        NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)\n    )\n\n    LOGGER.info(\"Setting uniqueness constraints on nodes\")\n    with driver.session(database=\"neo4j\") as session:\n        for node in NODES:\n            session.execute_write(_set_uniqueness_constraints, node)\n    # ...\n\n# ...\n</code></pre> <p>First, you define a helper function, _set_uniqueness_constraints(), that creates and runs queries enforcing each node to have a unique ID. In load_hospital_graph_from_csv(), you instantiate a driver that connects to your Neo4j instance and set uniqueness constraints for each hospital system node.</p> <p>Notice the @retry decorator attached to load_hospital_graph_from_csv(). If load_hospital_graph_from_csv() fails for any reason, this decorator will rerun it one hundred times with a ten second delay in between tries. This comes in handy when there are intermittent connection issues to Neo4j that are usually resolved by recreating a connection. However, be sure to check the script logs to see if an error reoccurs more than a few times.</p> <p>Next, load_hospital_graph_from_csv() loads data for each node and relationship:</p> <pre><code># ...\n\n@retry(tries=100, delay=10)\ndef load_hospital_graph_from_csv() -&gt; None:\n    \"\"\"Load structured hospital CSV data following\n    a specific ontology into Neo4j\"\"\"\n\n    # ...\n\n    LOGGER.info(\"Loading hospital nodes\")\n    with driver.session(database=\"neo4j\") as session:\n        query = f\"\"\"\n        LOAD CSV WITH HEADERS\n        FROM '{HOSPITALS_CSV_PATH}' AS hospitals\n        MERGE (h:Hospital {{id: toInteger(hospitals.hospital_id),\n                            name: hospitals.hospital_name,\n                            state_name: hospitals.hospital_state}});\n        \"\"\"\n        _ = session.run(query, {})\n\n   # ...\n\nif __name__ == \"__main__\":\n    load_hospital_graph_from_csv()\n</code></pre> <p>Each node and relationship is loaded from their respective csv files and written to Neo4j according to your graph database design. At the end of the script, you call load_hospital_graph_from_csv() in the name-main idiom, and all of the data should populate in your Neo4j instance.</p> <p>After writing hospital_neo4j_etl/src/hospital_bulk_csv_write.py, you can define an entrypoint.sh file that will run when your Docker container starts:</p> <pre><code>#!/bin/bash\n\n# Run any setup steps or pre-processing tasks here\necho \"Running ETL to move hospital data from csvs to Neo4j...\"\n\n# Run the ETL script\npython hospital_bulk_csv_write.py\n</code></pre> <p>This entrypoint file isn\u2019t technically necessary for this project, but it\u2019s a good practice when building containers because it allows you to execute necessary shell commands before running your main script.</p> <p>The last file to write for your ETL is the Docker file. It looks like this:</p> <pre><code>version: '3'\n\nservices:\n  hospital_neo4j_etl:\n    build:\n      context: ./hospital_neo4j_etl\n    env_file:\n      - .env\n</code></pre> <p>The ETL will run as a service called hospital_neo4j_etl, and it will run the Dockerfile in ./hospital_neo4j_etl using environment variables from .env. Since you only have one container, you don\u2019t need docker-compose yet. However, you\u2019ll add more containers to orchestrate with your ETL in the next section, so it\u2019s helpful to get started on docker-compose.yml.</p> <p>To run your ETL, open a terminal and run:</p> <pre><code> docker-compose up --build\n</code></pre> <p>Once the ETL finishes running, return to your Aura console: </p> <p>Click Open and you\u2019ll be prompted to enter your Neo4j password. After successfully logging into the instance, you should see a screen similar to this: </p> <p>As you can see under Database Information, all of the nodes, relationships, and properties were loaded. There are 21,187 nodes and 48,259 relationships. You\u2019re ready to start writing queries!</p>"},{"location":"rag_chatbot/chat_agent/","title":"Create the Chatbot Agent","text":"<p>Give yourself a pat on the back if you\u2019ve made it this far. You\u2019ve covered a lot of information, and you\u2019re finally ready to piece it all together and assemble the agent that will serve as your chatbot. Depending on the query you give it, your agent needs to decide between your Cypher chain, reviews chain, and wait times functions.</p> <p>Start by loading your agent\u2019s dependencies, reading in the agent model name from an environment variable, and loading a prompt template from LangChain Hub:</p> <pre><code>import os\nimport dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import (\n    create_openai_functions_agent,\n    Tool,\n    AgentExecutor,\n)\nfrom langchain import hub\nfrom chains.hospital_review_chain import reviews_vector_chain\nfrom chains.hospital_cypher_chain import hospital_cypher_chain\nfrom tools.wait_times import (\n    get_current_wait_times,\n    get_most_available_hospital,\n)\ndotenv.load_dotenv()\nHOSPITAL_AGENT_MODEL = os.getenv(\"HOSPITAL_AGENT_MODEL\")\n\nhospital_agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n</code></pre> <p>Notice how you\u2019re importing reviews_vector_chain, hospital_cypher_chain, get_current_wait_times(), and get_most_available_hospital(). Your agent will directly use these as tools. HOSPITAL_AGENT_MODEL is the LLM that will act as your agent\u2019s brain, deciding which tools to call and what inputs to pass them.</p> <p>Instead of defining your own prompt for the agent, which you can certainly do, you load a predefined prompt from LangChain Hub. LangChain hub lets you upload, browse, pull, test, and manage prompts. In this case, the default prompt for OpenAI function agents works great.</p> <p>Next, you define a list of tools your agent can use:</p> <pre><code># ...\n\ntools = [\n    Tool(\n        name=\"Experiences\",\n        func=reviews_vector_chain.invoke,\n        description=\"\"\"Useful when you need to answer questions\n        about patient experiences, feelings, or any other qualitative\n        question that could be answered about a patient using semantic\n        search. Not useful for answering objective questions that involve\n        counting, percentages, aggregations, or listing facts. Use the\n        entire prompt as input to the tool. For instance, if the prompt is\n        \"Are patients satisfied with their care?\", the input should be\n        \"Are patients satisfied with their care?\".\n        \"\"\",\n    ),\n    Tool(\n        name=\"Graph\",\n        func=hospital_cypher_chain.invoke,\n        description=\"\"\"Useful for answering questions about patients,\n        physicians, hospitals, insurance payers, patient review\n        statistics, and hospital visit details. Use the entire prompt as\n        input to the tool. For instance, if the prompt is \"How many visits\n        have there been?\", the input should be \"How many visits have\n        there been?\".\n        \"\"\",\n    ),\n    Tool(\n        name=\"Waits\",\n        func=get_current_wait_times,\n        description=\"\"\"Use when asked about current wait times\n        at a specific hospital. This tool can only get the current\n        wait time at a hospital and does not have any information about\n        aggregate or historical wait times. Do not pass the word \"hospital\"\n        as input, only the hospital name itself. For example, if the prompt\n        is \"What is the current wait time at Jordan Inc Hospital?\", the\n        input should be \"Jordan Inc\".\n        \"\"\",\n    ),\n    Tool(\n        name=\"Availability\",\n        func=get_most_available_hospital,\n        description=\"\"\"\n        Use when you need to find out which hospital has the shortest\n        wait time. This tool does not have any information about aggregate\n        or historical wait times. This tool returns a dictionary with the\n        hospital name as the key and the wait time in minutes as the value.\n        \"\"\",\n    ),\n]\n</code></pre> <p>Your agent has four tools available to it: Experiences, Graph, Waits, and Availability. The Experiences and Graph tools call .invoke() from their respective chains, while Waits and Availability call the wait time functions you defined. Notice that many of the tool descriptions have few-shot prompts, telling the agent when it should use the tool and providing it with an example of what inputs to pass.</p> <p>As with chains, good prompt engineering is crucial for your agent\u2019s success. You have to clearly describe each tool and how to use it so that your agent isn\u2019t confused by a query.</p> <p>The last step is to instantiate you agent:</p> <pre><code># ...\n\nchat_model = AzureChatOpenAI(\n    model=HOSPITAL_AGENT_MODEL,\n    temperature=0,\n)\n\nhospital_rag_agent = create_openai_functions_agent(\n    llm=chat_model,\n    prompt=hospital_agent_prompt,\n    tools=tools,\n)\n\nhospital_rag_agent_executor = AgentExecutor(\n    agent=hospital_rag_agent,\n    tools=tools,\n    return_intermediate_steps=True,\n    verbose=True,\n)\n</code></pre> <p>You first initialize a AzureChatOpenAI object using HOSPITAL_AGENT_MODEL as the LLM. You then create an AzureOpenAI functions agent with create_openai_functions_agent(). This creates an agent that\u2019s been designed by AzureOpenAI to pass inputs to functions. It does this by returning JSON objects that store function inputs and their corresponding value.</p> <p>To create the agent run time, you pass your agent and tools into AgentExecutor. Setting return_intermediate_steps and verbose to true allows you to see the agent\u2019s thought process and the tools it calls.</p> <p>With that, you\u2019ve completed building the hospital system agent. To try it out, you\u2019ll have to navigate into the chatbot_api/src/ folder and start a new REPL session from there.</p> <p>Note: This is necessary because you set up relative imports in hospital_rag_agent.py that\u2019ll later run within a Docker container. For now it means that you\u2019ll have to start your Python interpreter only after navigating into chatbot_api/src/ for the imports to work.</p> <p>You can now try out your hospital system agent on your command line:</p> <pre><code>import dotenv\ndotenv.load_dotenv()\n\nfrom agents.hospital_rag_agent import hospital_rag_agent_executor\n\nresponse = hospital_rag_agent_executor.invoke(\n    {\"input\": \"What is the wait time at Wallace-Hamilton?\"}\n)\nprint(response.get(\"output\"))\nresponse = hospital_rag_agent_executor.invoke(\n    {\"input\": \"Which hospital has the shortest wait time?\"}\n)\n\nprint(response.get(\"output\"))\n</code></pre> <p>After loading environment variables, you ask the agent about wait times. You can see exactly what it\u2019s doing in response to each of your queries. For instance, when you ask \u201cWhat is the wait time at Wallace-Hamilton?\u201d, it invokes the Wait tool and passes Wallace-Hamilton as input. This means the agent is calling get_current_wait_times(\"Wallace-Hamilton\"), observing the return value, and using the return value to answer your question.</p> <p>To see the agents full capabilities, you can ask it questions about patient experiences that require patient reviews to answer:</p> <pre><code>response = hospital_rag_agent_executor.invoke(\n    {\n        \"input\": (\n            \"What have patients said about their \"\n            \"quality of rest during their stay?\"\n        )\n    }\n)\nprint(response.get(\"output\"))\n</code></pre> <p>Notice here how you never explicitly mention reviews or experiences in your question. The agent knows, based on the tool description, that it needs to invoke Experiences. Lastly, you can ask the agent a question requiring a Cypher query to answer:</p> <pre><code>response = hospital_rag_agent_executor.invoke(\n    {\n        \"input\": (\n            \"Which physician has treated the \"\n            \"most patients covered by Cigna?\"\n        )\n    }\n)\n\nprint(response.get(\"output\"))\n</code></pre> <p>Your agent has a remarkable ability to know which tools to use and which inputs to pass based on your query. This is your fully-functioning chatbot. It has the potential to answer all the questions your stakeholders might ask based on the requirements given, and it appears to be doing a great job so far.</p> <p>As you ask your chatbot more questions, you\u2019ll almost certainly encounter situations where it calls the wrong tool or generates an incorrect answer. While modifying your prompts can help address incorrect answers, sometimes you can modify your input query to help your chatbot. Take a look at this example:</p> <pre><code>response = hospital_rag_agent_executor.invoke(\n    {\"input\": \"Show me reviews written by patient 7674.\"}\n)\n\nprint(response.get(\"output\"))\n</code></pre> <p>In this example, you ask the agent to show you reviews written by patient 7674. Your agent invokes Experiences and doesn\u2019t find the answer you\u2019re looking for. While it may be possible to find the answer using semantic vector search, you can get an exact answer by generating a Cypher query to look up reviews corresponding to patient ID 7674. To help your agent understand this, you can add additional detail to your query:</p> <pre><code>response = hospital_rag_agent_executor.invoke(\n    {\n        \"input\": (\n            \"Query the graph database to show me \"\n            \"the reviews written by patient 7674\"\n        )\n    }\n)\n\nresponse.get(\"output\")\n</code></pre> <p>Here, you explicitly tell your agent that you want to query the graph database, which correctly invokes Graph to find the review matching patient ID 7674. Providing more detail in your queries like this is a simple yet effective way to guide your agent when it\u2019s clearly invoking the wrong tools.</p> <p>As with your reviews and Cypher chain, before placing this in front of stakeholders, you\u2019d want to come up with a framework for evaluating your agent. The primary functionality you\u2019d want to evaluate is the agent\u2019s ability to call the correct tools with the correct inputs, and its ability to understand and interpret the outputs of the tools it calls.</p> <p>In the final step, you\u2019ll learn how to deploy your hospital system agent with FastAPI and Streamlit. This will make your agent accessible to anyone who calls the API endpoint or interacts with the Streamlit UI.</p>"},{"location":"rag_chatbot/introduction/","title":"Build a Graph RAG Chatbot in LangChain","text":"<p>After all the preparatory design and data work you\u2019ve done so far, you\u2019re finally ready to build your chatbot! You\u2019ll likely notice that, with the hospital system data stored in Neo4j, and the power of LangChain abstractions, building your chatbot doesn\u2019t take much work. This is a common theme in AI and ML projects\u2014most of the work is in design, data preparation, and deployment rather than building the AI itself.</p> <p>Before you jump in, add a chatbot_api/ folder to your project with the following files and folders:</p> <pre><code>./\n\u2502\n\u251c\u2500\u2500 chatbot_api/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 agents/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 hospital_rag_agent.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 chains/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 hospital_cypher_chain.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 hospital_review_chain.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 tools/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 wait_times.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 hospital_neo4j_etl/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 entrypoint.sh\n\u2502   \u2502   \u2514\u2500\u2500 hospital_bulk_csv_write.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 docker-compose.yml\n</code></pre> <p>You\u2019ll want to add a few more environment variables to your .env file as well:</p> <pre><code>NEO4J_URI=&lt;YOUR_NEO4J_URI&gt;\nNEO4J_USERNAME=&lt;YOUR_NEO4J_URI&gt;\nNEO4J_PASSWORD=&lt;YOUR_NEO4J_PASSWORD&gt;\n\nHOSPITALS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/hospitals.csv\nPAYERS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/payers.csv\nPHYSICIANS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/physicians.csv\nPATIENTS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/patients.csv\nVISITS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/visits.csv\nREVIEWS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/reviews.csv\n\nHOSPITAL_AGENT_MODEL=gpt-4-turbo\nHOSPITAL_CYPHER_MODEL=gpt-4-turbo\nHOSPITAL_QA_MODEL=gpt-4-turbo\n</code></pre> <p>Your .env file now includes variables that specify which LLM you\u2019ll use for different components of your chatbot. You\u2019ve specified these models as environment variables so that you can easily switch between different Azure OpenAI models without changing any code. Keep in mind, however, that each LLM might benefit from a unique prompting strategy, so you might need to modify your prompts if you plan on using a different suite of LLMs.</p> <p>You should already have the hospital_neo4j_etl/ folder completed, and docker-compose.yml and .env are the same as before. Open up chatbot_api/pyproject.toml and add the following dependencies:</p> <pre><code>[project]\nname = \"chatbot_api\"\nversion = \"0.1\"\ndependencies = [\n    \"asyncio==3.4.3\",\n    \"fastapi==0.109.0\",\n    \"langchain==0.1.0\",\n    \"langchain-openai==0.0.2\",\n    \"langchainhub==0.1.14\",\n    \"neo4j==5.14.1\",\n    \"numpy==1.26.2\",\n    \"openai==1.7.2\",\n    \"opentelemetry-api==1.22.0\",\n    \"pydantic==2.5.1\",\n    \"uvicorn==0.25.0\"\n]\n\n[project.optional-dependencies]\ndev = [\"black\", \"flake8\"]\n</code></pre> <p>You can certainly use more recent versions of these dependencies if they\u2019re available, but keep in mind any features that might be deprecated. Open a terminal, activate your virtual environment, navigate into your chatbot_api/ folder, and install dependencies from the project\u2019s pyproject.toml:</p> <pre><code>python -m pip install .\n</code></pre> <p>Once everything is installed, you\u2019re ready to build the reviews chain!</p>"},{"location":"rag_chatbot/neo_cypher_chain/","title":"Create a Neo4j Cypher Chain","text":"<p>As you saw in Step 2, your Neo4j Cypher chain will accept a user\u2019s natural language query, convert the natural language query to a Cypher query, run the Cypher query in Neo4j, and use the Cypher query results to respond to the user\u2019s query. You\u2019ll leverage LangChain\u2019s GraphCypherQAChain for this.</p> <p>Note: Any time you allow users to query a database, as you\u2019ll do with your Cypher chain, you need to ensure they only have necessary permissions. The Neo4j credentials you\u2019re using in this project allow users to read, write, update, and delete data from your database.</p> <p>If you were building this application for a real-world project, you\u2019d want to create credentials that restrict your user\u2019s permissions to reads only, preventing them from writing or deleting valuable data.</p> <p>Using LLMs to generate accurate Cypher queries can be challenging, especially if you have a complicated graph. Because of this, a lot of prompt engineering is required to show your graph structure and query use-cases to the LLM. Fine-tuning an LLM to generate queries is also an option, but this requires manually curated and labeled data.</p> <p>To get started creating your Cypher generation chain, import dependencies and instantiate a Neo4jGraph:</p> <pre><code>import os\nfrom langchain_community.graphs import Neo4jGraph\nfrom langchain.chains import GraphCypherQAChain\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\n\nHOSPITAL_QA_MODEL = os.getenv(\"HOSPITAL_QA_MODEL\")\nHOSPITAL_CYPHER_MODEL = os.getenv(\"HOSPITAL_CYPHER_MODEL\")\n\ngraph = Neo4jGraph(\n    url=os.getenv(\"NEO4J_URI\"),\n    username=os.getenv(\"NEO4J_USERNAME\"),\n    password=os.getenv(\"NEO4J_PASSWORD\"),\n)\n\ngraph.refresh_schema()\n</code></pre> <p>The Neo4jGraph object is a LangChain wrapper that allows LLMs to execute queries on your Neo4j instance. You instantiate graph using your Neo4j credentials, and you call graph.refresh_schema() to sync any recent changes to your instance.</p> <p>The next and most important component of your Cypher generation chain is the prompt template. Here\u2019s what that looks like:</p> <pre><code># ...\n\ncypher_generation_template = \"\"\"\nTask:\nGenerate Cypher query for a Neo4j graph database.\n\nInstructions:\nUse only the provided relationship types and properties in the schema.\nDo not use any other relationship types or properties that are not provided.\n\nSchema:\n{schema}\n\nNote:\nDo not include any explanations or apologies in your responses.\nDo not respond to any questions that might ask anything other than\nfor you to construct a Cypher statement. Do not include any text except\nthe generated Cypher statement. Make sure the direction of the relationship is\ncorrect in your queries. Make sure you alias both entities and relationships\nproperly. Do not run any queries that would add to or delete from\nthe database. Make sure to alias all statements that follow as with\nstatement (e.g. WITH v as visit, c.billing_amount as billing_amount)\nIf you need to divide numbers, make sure to\nfilter the denominator to be non zero.\n\nExamples:\n# Who is the oldest patient and how old are they?\nMATCH (p:Patient)\nRETURN p.name AS oldest_patient,\n       duration.between(date(p.dob), date()).years AS age\nORDER BY age DESC\nLIMIT 1\n\n# Which physician has billed the least to Cigna\nMATCH (p:Payer)&lt;-[c:COVERED_BY]-(v:Visit)-[t:TREATS]-(phy:Physician)\nWHERE p.name = 'Cigna'\nRETURN phy.name AS physician_name, SUM(c.billing_amount) AS total_billed\nORDER BY total_billed\nLIMIT 1\n\n# Which state had the largest percent increase in Cigna visits\n# from 2022 to 2023?\nMATCH (h:Hospital)&lt;-[:AT]-(v:Visit)-[:COVERED_BY]-&gt;(p:Payer)\nWHERE p.name = 'Cigna' AND v.admission_date &gt;= '2022-01-01' AND\nv.admission_date &lt; '2024-01-01'\nWITH h.state_name AS state, COUNT(v) AS visit_count,\n     SUM(CASE WHEN v.admission_date &gt;= '2022-01-01' AND\n     v.admission_date &lt; '2023-01-01' THEN 1 ELSE 0 END) AS count_2022,\n     SUM(CASE WHEN v.admission_date &gt;= '2023-01-01' AND\n     v.admission_date &lt; '2024-01-01' THEN 1 ELSE 0 END) AS count_2023\nWITH state, visit_count, count_2022, count_2023,\n     (toFloat(count_2023) - toFloat(count_2022)) / toFloat(count_2022) * 100\n     AS percent_increase\nRETURN state, percent_increase\nORDER BY percent_increase DESC\nLIMIT 1\n\n# How many non-emergency patients in North Carolina have written reviews?\nMATCH (r:Review)&lt;-[:WRITES]-(v:Visit)-[:AT]-&gt;(h:Hospital)\nWHERE h.state_name = 'NC' and v.admission_type &lt;&gt; 'Emergency'\nRETURN count(*)\n\nString category values:\nTest results are one of: 'Inconclusive', 'Normal', 'Abnormal'\nVisit statuses are one of: 'OPEN', 'DISCHARGED'\nAdmission Types are one of: 'Elective', 'Emergency', 'Urgent'\nPayer names are one of: 'Cigna', 'Blue Cross', 'UnitedHealthcare', 'Medicare',\n'Aetna'\n\nA visit is considered open if its status is 'OPEN' and the discharge date is\nmissing.\nUse abbreviations when\nfiltering on hospital states (e.g. \"Texas\" is \"TX\",\n\"Colorado\" is \"CO\", \"North Carolina\" is \"NC\",\n\"Florida\" is \"FL\", \"Georgia\" is \"GA\", etc.)\n\nMake sure to use IS NULL or IS NOT NULL when analyzing missing properties.\nNever return embedding properties in your queries. You must never include the\nstatement \"GROUP BY\" in your query. Make sure to alias all statements that\nfollow as with statement (e.g. WITH v as visit, c.billing_amount as\nbilling_amount)\nIf you need to divide numbers, make sure to filter the denominator to be non\nzero.\n\nThe question is:\n{question}\n\"\"\"\n\ncypher_generation_prompt = PromptTemplate(\n    input_variables=[\"schema\", \"question\"], template=cypher_generation_template\n)\n</code></pre> <p>Read the contents of cypher_generation_template carefully. Notice how you\u2019re providing the LLM with very specific instructions on what it should and shouldn\u2019t do when generating Cypher queries. Most importantly, you\u2019re showing the LLM your graph\u2019s structure with the schema parameter, some example queries, and the categorical values of a few node properties.</p> <p>All of the detail you provide in your prompt template improves the LLM\u2019s chance of generating a correct Cypher query for a given question. If you\u2019re curious about how necessary all this detail is, try creating your own prompt template with as few details as possible. Then run questions through your Cypher chain and see whether it correctly generates Cypher queries.</p> <p>From there, you can iteratively update your prompt template to correct for queries that the LLM struggles to generate, but make sure you\u2019re also cognizant of the number of input tokens you\u2019re using. As with your review chain, you\u2019ll want a solid system for evaluating prompt templates and the correctness of your chain\u2019s generated Cypher queries. However, as you\u2019ll see, the template you have above is a great starting place.</p> <p>Note: The above prompt template provides the LLM with four examples of valid Cypher queries for your graph. Giving the LLM a few examples and then asking it to perform a task is known as few-shot prompting, and it\u2019s a simple yet powerful technique for improving generation accuracy.</p> <p>However, few-shot prompting might not be sufficient for Cypher query generation, especially if you have a complicated graph. One way to improve this is to create a vector database that embeds example user questions/queries and stores their corresponding Cypher queries as metadata.</p> <p>When a user asks a question, you inject Cypher queries from semantically similar questions into the prompt, providing the LLM with the most relevant examples needed to answer the current question.</p> <p>Next, you define the prompt template for the question-answer component of your chain. This template tells the LLM to use the Cypher query results to generate a nicely-formatted answer to the user\u2019s query:</p> <pre><code># ...\n\nqa_generation_template = \"\"\"You are an assistant that takes the results\nfrom a Neo4j Cypher query and forms a human-readable response. The\nquery results section contains the results of a Cypher query that was\ngenerated based on a user's natural language question. The provided\ninformation is authoritative, you must never doubt it or try to use\nyour internal knowledge to correct it. Make the answer sound like a\nresponse to the question.\n\nQuery Results:\n{context}\n\nQuestion:\n{question}\n\nIf the provided information is empty, say you don't know the answer.\nEmpty information looks like this: []\n\nIf the information is not empty, you must provide an answer using the\nresults. If the question involves a time duration, assume the query\nresults are in units of days unless otherwise specified.\n\nWhen names are provided in the query results, such as hospital names,\nbeware  of any names that have commas or other punctuation in them.\nFor instance, 'Jones, Brown and Murray' is a single hospital name,\nnot multiple hospitals. Make sure you return any list of names in\na way that isn't ambiguous and allows someone to tell what the full\nnames are.\n\nNever say you don't have the right information if there is data in\nthe query results. Always use the data in the query results.\n\nHelpful Answer:\n\"\"\"\n\nqa_generation_prompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"], template=qa_generation_template\n)\n</code></pre> <p>This template requires much less detail than your Cypher generation template, and you should only have to modify it if you want the LLM to respond differently, or if you\u2019re noticing that it\u2019s not using the query results how you want. The last step in creating your Cypher chain is to instantiate a GraphCypherQAChain object:</p> <pre><code># ...\n\nhospital_cypher_chain = GraphCypherQAChain.from_llm(\n    cypher_llm=ChatOpenAI(model=HOSPITAL_CYPHER_MODEL, temperature=0),\n    qa_llm=ChatOpenAI(model=HOSPITAL_QA_MODEL, temperature=0),\n    graph=graph,\n    verbose=True,\n    qa_prompt=qa_generation_prompt,\n    cypher_prompt=cypher_generation_prompt,\n    validate_cypher=True,\n    top_k=100,\n)\n</code></pre> <p>Here\u2019s a breakdown of the parameters used in GraphCypherQAChain.from_llm():</p> <ul> <li>cypher_llm: The LLM used to generate Cypher queries.</li> <li>qa_llm: The LLM used to generate an answer given Cypher query results.</li> <li>graph: The Neo4jGraph object that connects to your Neo4j instance.</li> <li>verbose: Whether intermediate steps your chain performs should be printed.</li> <li>qa_prompt: The prompt template for responding to questions/queries.</li> <li>cypher_prompt: The prompt template for generating Cypher queries.</li> <li>validate_cypher: If true, the Cypher query will be inspected for errors and corrected before running. Note that this doesn\u2019t guarantee the Cypher query will be valid. Instead, it corrects simple syntax errors that are easily detectable using regular expressions.</li> <li>top_k: The number of query results to include in qa_prompt.</li> </ul> <p>Your hospital system Cypher generation chain is ready to use! It works the same way as your reviews chain. Navigate to your project directory and start a new Python interpreter session, then give it a try:</p> <pre><code>import dotenv\ndotenv.load_dotenv()\n\n\nfrom chatbot_api.src.chains.hospital_cypher_chain import (\nhospital_cypher_chain\n)\n\nquestion = \"\"\"What is the average visit duration for\nemergency visits in North Carolina?\"\"\"\nresponse = hospital_cypher_chain.invoke(question)\nprint(response.get(\"result\"))\n</code></pre> <p>After loading environment variables, importing hospital_cypher_chain, and invoking it with a question, you can see the steps your chain takes to answer the question. Take a second to appreciate a few accomplishments your chain made when generating the Cypher query:</p> <ul> <li>The Cypher generation LLM understood the relationship between visits and hospitals from the provided graph schema.</li> <li>Even though you asked it about North Carolina, the LLM knew from the prompt to use the state abbreviation NC.</li> <li>The LLM knew that admission_type properties only have the first letter capitalized, while the status properties are all caps.</li> <li>The QA generation LLM knew from your prompt that the query results were in units of days.</li> </ul> <p>You can experiment with all kinds of queries about the hospital system. For example, here\u2019s a relatively challenging question to convert to Cypher:</p> <pre><code>question = \"\"\"Which state had the largest percent increase\n           in Medicaid visits from 2022 to 2023?\"\"\"\nresponse = hospital_cypher_chain.invoke(question)\nresponse.get(\"result\")\n</code></pre> <p>To answer the question Which state had the largest percent increase in Medicaid visits from 2022 to 2023?, the LLM had to generate a fairly verbose Cypher query involving multiple nodes, relationships, and filters. Nonetheless, it was able to arrive at the correct answer.</p> <p>The last capability your chatbot needs is to answer questions about wait times, and that\u2019s what you\u2019ll cover next.</p>"},{"location":"rag_chatbot/neo_vector_chain/","title":"Create a Neo4j Vector Chain","text":"<p>In Step 1, you got a hands-on introduction to LangChain by building a chain that answers questions about patient experiences using their reviews. In this section, you\u2019ll build a similar chain except you\u2019ll use Neo4j as your vector index.</p> <p>Vector search indexes were released as a public beta in Neo4j 5.11. They allow you to run semantic queries directly on your graph. This is really convenient for your chatbot because you can store review embeddings in the same place as your structured hospital system data.</p> <p>In LangChain, you can use Neo4jVector to create review embeddings and the retriever needed for your chain. Here\u2019s the code to create the reviews chain:</p> <pre><code>import os\n\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    PromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain.vectorstores.neo4j_vector import Neo4jVector\nfrom langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\nimport dotenv\n\ndotenv.load_dotenv()\n\nHOSPITAL_QA_MODEL = os.getenv(\"HOSPITAL_QA_MODEL\")\n\nembd = AzureOpenAIEmbeddings(\n    azure_deployment=\"text-embedding-ada-002\",\n    openai_api_version=\"2023-05-15\",\n)\n\nneo4j_vector_index = Neo4jVector.from_existing_graph(\n    embedding=embd,\n    url=os.getenv(\"NEO4J_URI\"),\n    username=os.getenv(\"NEO4J_USERNAME\"),\n    password=os.getenv(\"NEO4J_PASSWORD\"),\n    index_name=\"reviews\",\n    node_label=\"Review\",\n    text_node_properties=[\n        \"physician_name\",\n        \"patient_name\",\n        \"text\",\n        \"hospital_name\",\n    ],\n    embedding_node_property=\"embedding\",\n)\n\nreview_template = \"\"\"Your job is to use patient\nreviews to answer questions about their experience at\na hospital. Use the following context to answer questions.\nBe as detailed as possible, but don't make up any information\nthat's not from the context. If you don't know an answer,\nsay you don't know.\n{context}\n\"\"\"\n\nreview_system_prompt = SystemMessagePromptTemplate(\n    prompt=PromptTemplate(input_variables=[\"context\"], template=review_template)\n)\n\nreview_human_prompt = HumanMessagePromptTemplate(\n    prompt=PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n)\nmessages = [review_system_prompt, review_human_prompt]\n\nreview_prompt = ChatPromptTemplate(\n    input_variables=[\"context\", \"question\"], messages=messages\n)\n\nreviews_vector_chain = RetrievalQA.from_chain_type(\n    llm=AzureChatOpenAI(model=HOSPITAL_QA_MODEL, temperature=0),\n    chain_type=\"stuff\",\n    retriever=neo4j_vector_index.as_retriever(k=12),\n)\nreviews_vector_chain.combine_documents_chain.llm_chain.prompt = review_prompt\n</code></pre> <p>you import the dependencies needed to build your review chain with Neo4j. You load the name of the chat model you\u2019ll use for the review chain and store it in HOSPITAL_QA_MODEL. Create the vector index in Neo4j. Here\u2019s a breakdown of each parameter:</p> <ul> <li>embedding: The model used to create the embeddings\u2014you\u2019re using AzureOpenAIEmeddings() in this example.</li> <li>url, username, and password: Your Neo4j instance credentials.</li> <li>index_name: The name given to your vector index.</li> <li>node_label: The node to create embeddings for.</li> <li>text_node_properties: The node properties to include in the embedding.</li> <li>embedding_node_property: The name of the embedding node property.</li> </ul> <p>Once Neo4jVector.from_existing_graph() runs, you\u2019ll see that every Review node in Neo4j has an embedding property which is a vector representation of the physician_name, patient_name, text, and hospital_name properties. This allows you to answer questions like Which hospitals have had positive reviews? It also allows the LLM to tell you which patient and physician wrote reviews matching your question.</p> <p>Create the prompt template for your review chain the same way you did in Step 1.</p> <p>Lastly, Create your reviews vector chain using a Neo4j vector index retriever that returns 12 reviews embeddings from a similarity search. By setting chain_type to \"stuff\" in .from_chain_type(), you\u2019re telling the chain to pass all 12 reviews to the prompt. You can explore other chain types in LangChain\u2019s documentation on chains.</p> <p>You\u2019re ready to try out your new reviews chain. Navigate to the root directory of your project, start a Python interpreter, and run the following commands:</p> <pre><code>import dotenv\ndotenv.load_dotenv()\n\n\nfrom chatbot_api.src.chains.hospital_review_chain import (\n    reviews_vector_chain\n)\n\nquery = \"\"\"What have patients said about hospital efficiency?\n        Mention details from specific reviews.\"\"\"\n\nresponse = reviews_vector_chain.invoke(query)\n\nresponse.get(\"result\")\n</code></pre> <p>In this block, you import dotenv and load environment variables from .env. You then import reviews_vector_chain from hospital_review_chain and invoke it with a question about hospital efficiency. Your chain\u2019s response might not be identical to this, but the LLM should return a nice detailed summary, as you\u2019ve told it to.</p> <p>In this example, notice how specific patient and hospital names are mentioned in the response. This happens because you embedded hospital and patient names along with the review text, so the LLM can use this information to answer questions.</p> <p>Note: Before moving on, you should play around with reviews_vector_chain to see how it responds to different queries. Do the responses seem correct? How might you evaluate the quality of reviews_vector_chain? You won\u2019t learn how to evaluate RAG systems in this tutorial, but you can look at this comprehensive Python example with MLFlow to get a feel for how it\u2019s done.</p> <p>Next up, you\u2019ll create the Cypher generation chain that you\u2019ll use to answer queries about structured hospital system data.</p>"},{"location":"rag_chatbot/wait_time/","title":"Create Wait Time Functions","text":"<p>This last capability your chatbot needs is to answer questions about hospital wait times. As discussed earlier, your organization doesn\u2019t store wait time data anywhere, so your chatbot will have to fetch it from an external source. You\u2019ll write two functions for this\u2014one that simulates finding the current wait time at a hospital, and another that finds the hospital with the shortest wait time.</p> <p>Note: The purpose of creating wait time functions is to show you that LangChain agents can run arbitrary Python code, not just chains or other LangChain methods. This capability is extremely valuable because it means, in theory, you could create an agent to do just about anything that can be expressed in code.</p> <p>Start by defining functions to fetch current wait times at a hospital:</p> <pre><code>import os\nfrom typing import Any\nimport dotenv\n\nimport numpy as np\nfrom langchain_community.graphs import Neo4jGraph\n\ndotenv.load_dotenv()\n\n\ndef _get_current_hospitals() -&gt; list[str]:\n    \"\"\"Fetch a list of current hospital names from a Neo4j database.\"\"\"\n    graph = Neo4jGraph(\n        url=os.getenv(\"NEO4J_URI\"),\n        username=os.getenv(\"NEO4J_USERNAME\"),\n        password=os.getenv(\"NEO4J_PASSWORD\"),\n    )\n\n    current_hospitals = graph.query(\n        \"\"\"\n        MATCH (h:Hospital)\n        RETURN h.name AS hospital_name\n        \"\"\"\n    )\n\n    current_hospitals = [d[\"hospital_name\"].lower() for d in current_hospitals]\n\n    return current_hospitals\n\n\ndef _get_current_wait_time_minutes(hospital: str) -&gt; int:\n    \"\"\"Get the current wait time at a hospital in minutes.\"\"\"\n\n    current_hospitals = _get_current_hospitals()\n\n    if hospital.lower() not in current_hospitals:\n        return -1\n\n    return np.random.randint(low=0, high=600)\n\n\ndef get_current_wait_times(hospital: str) -&gt; str:\n    \"\"\"Get the current wait time at a hospital formatted as a string.\"\"\"\n\n    wait_time_in_minutes = _get_current_wait_time_minutes(hospital)\n\n    if wait_time_in_minutes == -1:\n        return f\"Hospital '{hospital}' does not exist.\"\n\n    hours, minutes = divmod(wait_time_in_minutes, 60)\n\n    if hours &gt; 0:\n        formatted_wait_time = f\"{hours} hours {minutes} minutes\"\n    else:\n        formatted_wait_time = f\"{minutes} minutes\"\n\n    return formatted_wait_time\n\n\ndef get_most_available_hospital(_: Any) -&gt; dict[str, float]:\n    \"\"\"Find the hospital with the shortest wait time.\"\"\"\n\n    current_hospitals = _get_current_hospitals()\n\n    current_wait_times = [_get_current_wait_time_minutes(h) for h in current_hospitals]\n\n    best_time_idx = np.argmin(current_wait_times)\n    best_hospital = current_hospitals[best_time_idx]\n    best_wait_time = current_wait_times[best_time_idx]\n\n    return {best_hospital: best_wait_time}\n</code></pre> <p>The first function you define is _get_current_hospitals() which returns a list of hospital names from your Neo4j database. Then, _get_current_wait_time_minutes() takes a hospital name as input. If the hospital name is invalid, _get_current_wait_time_minutes() returns -1. If the hospital name is valid, _get_current_wait_time_minutes() returns a random integer between 0 and 600 simulating a wait time in minutes.</p> <p>You then define get_current_wait_times() which is a wrapper around _get_current_wait_time_minutes() that returns the wait time formatted as a string.</p> <p>You can use _get_current_wait_time_minutes() to define a second function that finds the hospital with the shortest wait time:</p> <pre><code># ...\n\ndef get_most_available_hospital(_: Any) -&gt; dict[str, float]:\n    \"\"\"Find the hospital with the shortest wait time.\"\"\"\n    current_hospitals = _get_current_hospitals()\n\n    current_wait_times = [\n        _get_current_wait_time_minutes(h) for h in current_hospitals\n    ]\n\n    best_time_idx = np.argmin(current_wait_times)\n    best_hospital = current_hospitals[best_time_idx]\n    best_wait_time = current_wait_times[best_time_idx]\n\n    return {best_hospital: best_wait_time}\n</code></pre> <p>Here, you define get*mostavailable_hospital() which calls _get_current_wait_time_minutes() on each hospital and returns the hospital with the shortest wait time. Notice how get_most_available_hospital() has a throwaway input. This will be required later on by your agent because it\u2019s designed to pass inputs into functions.</p> <p>Here\u2019s how you use get_current_wait_times() and get_most_available_hospital():</p> <pre><code>import dotenv\n\ndotenv.load_dotenv()\n\n\nfrom wait_times import (\n    get_current_wait_times,\n    get_most_available_hospital,\n)\n\nprint(get_current_wait_times(\"Wallace-Hamilton\"))\n\n\nprint(get_current_wait_times(\"fake hospital\"))\n\n\nprint(get_most_available_hospital(None))\n</code></pre> <p>After loading environment variables, you call get_current_wait_times(\"Wallace-Hamilton\") which returns the current wait time in minutes at Wallace-Hamilton hospital. When you try get_current_wait_times(\"fake hospital\"), you get a string telling you fake hospital does not exist in the database.</p> <p>Lastly, get_most_available_hospital() returns a dictionary storing the wait time for the hospital with the shortest wait time in minutes. Next, you\u2019ll create an agent that uses these functions, along with the Cypher and review chain, to answer arbitrary questions about the hospital system.</p>"},{"location":"tags/","title":"Tags","text":"<p>This page shows a list of pages indexed by their tags:</p>"}]}